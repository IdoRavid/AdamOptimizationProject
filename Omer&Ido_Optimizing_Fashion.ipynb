{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "osfFbOiTGXn0"
   },
   "source": [
    "# Setup\n",
    "Imports, data loading, model definition, and oracle functions."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "B1ZW1kjTFO24",
    "outputId": "f7e3d4e8-1d20-478e-ce46-d5e03e31f6aa",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "source": "# Clone code from GitHub\n!git clone https://github.com/IdoRavid/AdamOptimizationProject.git /content/project 2>/dev/null || (cd /content/project && git pull)\n%cd /content/project\n\nimport sys\nsys.path.insert(0, 'src')\n",
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "/content/project\n",
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "form",
    "id": "eKkkKJ0qDYb8"
   },
   "outputs": [],
   "source": [
    "# @title Imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yft3XDWxDTPg",
    "outputId": "ebae20d5-8e91-4b27-9556-aa2befedd27c"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loading Data...\n"
     ]
    }
   ],
   "source": [
    "# @title Data Loader\n",
    "print(\"Loading Data...\")\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "train_data = datasets.FashionMNIST(root='./data', train=True, transform=transform, download=True)\n",
    "test_data = datasets.FashionMNIST(root='./data', train=False, transform=transform, download=True)\n",
    "x_train = train_data.data\n",
    "x_test = test_data.data\n",
    "image_size = x_train.shape[1] * x_train.shape[2]\n",
    "x_train = x_train.reshape(-1, image_size).float() / 255\n",
    "x_test = x_test.reshape(-1, image_size).float() / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "ykXQg9hDDor7",
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "# @title AutoEncoder\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(image_size, 256), nn.ReLU(),\n",
    "            nn.Linear(256, 64), nn.ReLU(),\n",
    "            nn.Linear(64, 32)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(32, 64), nn.ReLU(),\n",
    "            nn.Linear(64, 128), nn.ReLU(),\n",
    "            nn.Linear(128, image_size), nn.Sigmoid()\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q_ill-I0HBJb"
   },
   "outputs": [],
   "source": [
    "#init autoencoder\n",
    "autoencoder = Autoencoder()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bakHXzqJa_eL"
   },
   "source": [
    "## oracle functions\n",
    "you should use to given model, loss function and x returns either the model output, the loss, the gradient and the hessian."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "-20Y4GO_ECuk"
   },
   "outputs": [],
   "source": [
    "def autoencoder_oracle(model, criterion, x, calc_hessian=False):\n",
    "    \"\"\"\n",
    "    Computes f(x), gradient, and Hessian for a given input x.\n",
    "    \"\"\"\n",
    "    # Forward Pass\n",
    "    reconstructed_x = model(x)\n",
    "    loss = criterion(reconstructed_x, x)\n",
    "\n",
    "    grads = torch.autograd.grad(loss, model.parameters(), create_graph=calc_hessian)\n",
    "\n",
    "    hessians = []\n",
    "\n",
    "    if calc_hessian:\n",
    "        for i, (grad, param) in enumerate(zip(grads, model.parameters())):\n",
    "\n",
    "            grad_flat = grad.view(-1)\n",
    "\n",
    "            hessian_rows = []\n",
    "            for j in range(len(grad_flat)):\n",
    "\n",
    "                grad_2nd = torch.autograd.grad(grad_flat[j], param, retain_graph=True)[0]\n",
    "                hessian_rows.append(grad_2nd.view(-1))\n",
    "\n",
    "            hessian_matrix = torch.stack(hessian_rows)\n",
    "            hessians.append(hessian_matrix)\n",
    "\n",
    "\n",
    "    return loss, grads, hessians"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Fu0KkS3fD4Nm",
    "outputId": "541d9926-650b-4d62-f988-f7a2f262fb01"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Initializing the model (Defining the function surface)...\n"
     ]
    }
   ],
   "source": [
    "print(\"Initializing the model (Defining the function surface)...\")\n",
    "train_dataset = TensorDataset(x_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# This dataset is for the second order learners - so you wont run out of RAM\n",
    "small_train_dataset = TensorDataset(x_train[:100]) # \u05e8\u05e7 100 \u05ea\u05de\u05d5\u05e0\u05d5\u05ea \u05dc\u05d4\u05d3\u05d2\u05de\u05d4\n",
    "small_train_loader = DataLoader(small_train_dataset, batch_size=10, shuffle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bf4b5052",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "outputId": "72368616-1a79-4cec-dcda-efdfde5cb0c9"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Visualizing Results...\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<Figure size 800x1500 with 10 Axes>"
      ],
      "image/png": "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\n"
     },
     "metadata": {}
    }
   ],
   "source": "def show_model_reconstructions(model, batch_images, count=5):\n    \"\"\"\n    Visualizes original images vs. model reconstructions.\n    \"\"\"\n    print(\"\\nVisualizing Results...\")\n\n    model.eval()\n\n    with torch.no_grad():\n        inputs = batch_images[:count]\n        reconstructions = model(inputs)\n\n    plt.figure(figsize=(8, 3 * count))\n\n    for i in range(count):\n        img_orig = inputs[i].cpu().numpy().reshape(28, 28)\n        img_recon = reconstructions[i].cpu().numpy().reshape(28, 28)\n\n        # Original\n        ax = plt.subplot(count, 2, i*2 + 1)\n        plt.imshow(img_orig, cmap='gray')\n        if i == 0: ax.set_title(\"Original\")\n        plt.axis('off')\n\n        #Reconstructed\n        ax = plt.subplot(count, 2, i*2 + 2)\n        plt.imshow(img_recon, cmap='gray')\n        if i == 0: ax.set_title(\"Reconstructed (Model Output)\")\n        plt.axis('off')\n\n    plt.tight_layout()\n    plt.show()\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "Analysis visualizations for Phase 1 & Phase 2 results (loaded from git repo data)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# @title Phase 1 - Grid Search Heatmaps\nfrom utils.experiment import OptimizerExperiment\nimport matplotlib.pyplot as plt\nimport json\nimport os\n\nRESULTS_DIR = 'results/phase1'\n\n# Load Phase 1 results (OptimizerExperiment format only)\nresults = {}\nfor fname in os.listdir(RESULTS_DIR):\n    if fname.endswith('.json'):\n        path = os.path.join(RESULTS_DIR, fname)\n        with open(path) as f:\n            data = json.load(f)\n        if 'optimizer_id' in data:  # Skip E* files with different format\n            exp = OptimizerExperiment.load(path)\n            results[exp.optimizer_id] = exp\n            print(f\"Loaded {exp.optimizer_id}\")\n\n# Plot heatmaps\nfor variant_id, exp in sorted(results.items()):\n    exp.plot_heatmap()\n    plt.show()\n",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# @title Phase 2 Visualization\nimport matplotlib.pyplot as plt\nimport json\n\n# Load results from git (A1, B1, B3)\nwith open('results/phase2/phase2_results.json') as f:\n    phase2_results = json.load(f)\n\n# Load E1, E2 results\nwith open('results/phase2/phase2_E1_2_results.json') as f:\n    e_results = json.load(f)\n    phase2_results.update(e_results)\n\ndef plot_phase2_curves(results, metric='test_loss'):\n    variants = list(set(name.split('_')[0] for name in results.keys()))\n    fig, axes = plt.subplots(1, len(variants), figsize=(5*len(variants), 4))\n    if len(variants) == 1:\n        axes = [axes]\n    for ax, variant in zip(axes, sorted(variants)):\n        for name, hist in results.items():\n            if name.startswith(variant + '_'):\n                label = f\"lr={hist['config']['lr_mult']}, wd={hist['config']['wd_mult']}\"\n                ax.plot(hist[metric], label=label, alpha=0.8)\n        ax.set_title(f'{variant} - {metric}')\n        ax.set_xlabel('Epoch')\n        ax.set_ylabel(metric)\n        ax.legend(fontsize=8)\n        ax.grid(alpha=0.3)\n    plt.tight_layout()\n    plt.show()\n\nplot_phase2_curves(phase2_results, 'test_loss')\nplot_phase2_curves(phase2_results, 'train_loss')\nplot_phase2_curves(phase2_results, 'grad_norm')\nplot_phase2_curves(phase2_results, 'weight_norm')\n",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# @title Projection Experiments Visualization\nimport json\nimport matplotlib.pyplot as plt\nfrom IPython.display import Image, display\n\n# Display pre-generated projection analysis plot\nprint(\"Adafactor Projection Experiments: L2 Ball vs Box Constraints\")\ndisplay(Image('analysis/projection_experiments.png'))\n\n# Load and summarize projection test results\nPROJ_DIR = 'results/projection_test'\nimport os\n\nprint(\"\\nProjection Test Summary (30 epochs):\")\nprint(\"-\" * 60)\nfor fname in sorted(os.listdir(PROJ_DIR)):\n    if fname.endswith('.json'):\n        with open(f'{PROJ_DIR}/{fname}') as f:\n            data = json.load(f)\n        name = data.get('optimizer_name', fname)\n        results = data.get('results', [])\n        if results:\n            best = min(r['best_test_loss'] for r in results)\n            print(f\"{name}: best_test_loss = {best:.4f}\")\n",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# @title Image Reconstruction Through Training\nimport torch\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nimport matplotlib.pyplot as plt\n\nMODELS_DIR = 'results/models'\nEPOCHS = [25, 50, 75, 100]\nNUM_IMAGES = 4\n\n# Best config per variant from Phase 2\nCONFIGS = {\n    'A1': 'lr2_wd0',\n    'B1': 'lr2_wd1', \n    'B3': 'lr2_wd0.25'\n}\n\n# Get fixed sample images\ndata_iter = iter(train_loader)\nsample_images = next(data_iter)[0][:NUM_IMAGES].to(device)\n\nfor img_idx in range(NUM_IMAGES):\n    fig, axes = plt.subplots(len(CONFIGS), len(EPOCHS) + 1, figsize=(12, 2.5 * len(CONFIGS)))\n    fig.suptitle(f'Image {img_idx + 1}', fontsize=14)\n    \n    for row, (variant, cfg) in enumerate(CONFIGS.items()):\n        # Show original in first column\n        ax = axes[row, 0]\n        ax.imshow(sample_images[img_idx].cpu().numpy().reshape(28, 28), cmap='gray')\n        ax.set_title('Original' if row == 0 else '')\n        ax.set_ylabel(variant)\n        ax.axis('off')\n        \n        # Show reconstructions at each epoch\n        for col, epoch in enumerate(EPOCHS, 1):\n            model = Autoencoder().to(device)\n            weights_path = f'{MODELS_DIR}/{variant}_{cfg}_epoch{epoch}.pt'\n            model.load_state_dict(torch.load(weights_path, map_location=device))\n            model.eval()\n            \n            with torch.no_grad():\n                recon = model(sample_images[img_idx:img_idx+1])\n            \n            ax = axes[row, col]\n            ax.imshow(recon[0].cpu().numpy().reshape(28, 28), cmap='gray')\n            ax.set_title(f'Ep{epoch}' if row == 0 else '')\n            ax.axis('off')\n    \n    plt.tight_layout()\n    plt.show()\n",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Model: B3 (AdamW Cosine) with LR=2, WD=0.25\n",
    "\n",
    "Based on our Phase 1 grid search and Phase 2 deep training experiments, we selected:\n",
    "- **Optimizer**: B3 (AdamW with Cosine LR schedule)\n",
    "- **Learning Rate**: 0.002 (base_lr \u00d7 2)\n",
    "- **Weight Decay**: 0.00025 (base_wd \u00d7 0.25)\n",
    "\n",
    "This configuration achieved the best balance of convergence speed, final loss, and reconstruction quality."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# @title Load Final Model and Run on New Data\nimport torch\nimport torch.nn as nn\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# Load the final trained model\nfinal_model = Autoencoder().to(device)\n\n# Model loading priority:\n# 1. From cloned GitHub repo (results/models/)\n# 2. From Google Drive (if mounted)\n# 3. From manual upload to /content/\n#\n# If options 1 & 2 fail, manually upload B3_lr2_wd0.25_epoch100.pt to /content/\n\nmodel_paths = [\n    'results/models/B3_lr2_wd0.25_epoch100.pt',  # From GitHub clone\n    '/content/drive/MyDrive/Optimization Project/results/phase2/B3_lr2_wd0.25_epoch100.pt',  # From Drive\n    '/content/B3_lr2_wd0.25_epoch100.pt',  # Manual upload fallback\n]\n\nloaded = False\nfor path in model_paths:\n    try:\n        final_model.load_state_dict(torch.load(path, map_location=device))\n        print(f\"Loaded final model from: {path}\")\n        loaded = True\n        break\n    except:\n        continue\n\nif not loaded:\n    raise FileNotFoundError(\"Could not load model. Please upload B3_lr2_wd0.25_epoch100.pt to /content/\")\n\nfinal_model.eval()\n\n# Evaluate on test set\ncriterion = nn.MSELoss()\ntest_loader = DataLoader(TensorDataset(x_test.to(device)), batch_size=128)\n\nwith torch.no_grad():\n    test_loss = sum(criterion(final_model(b[0]), b[0]).item() for b in test_loader) / len(test_loader)\nprint(f\"Final Model Test Loss: {test_loss:.6f}\")\n\n# Show reconstructions on test data\nprint(\"\\nReconstructions on Test Set:\")\nshow_model_reconstructions(final_model, x_test.to(device), count=5)\n",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "Run experiments and save results to Google Drive."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# @title Mount Google Drive\n# NOTE: Google Drive was used for our internal training experiments to save checkpoints\n# and results persistently across Colab sessions. For evaluation, pre-trained models\n# are available in the GitHub repository under results/models/\n\nfrom google.colab import drive\ndrive.mount('/content/drive')\n\nRESULTS_DIR = '/content/drive/MyDrive/Optimization Project/results/'\n!mkdir -p \"$RESULTS_DIR\"\nprint(f\"Results will be saved to: {RESULTS_DIR}\")\n",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lN29JrzwFO3a"
   },
   "source": [
    "## Custom Optimizer Implementation\n",
    "Variants: A1 (Adam+L2), B1-B8 (AdamW with different schedules)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "AUVzcHr_FO3a"
   },
   "source": [
    "# Import custom optimizers\n",
    "import sys\n",
    "sys.path.insert(0, 'src')\n",
    "\n",
    "from optimizers.base import OptimizerConfig\n",
    "from optimizers.adam_l2 import AdamL2\n",
    "from optimizers.adamw import AdamW, AdamWConfig\n",
    "from optimizers.schedulers import FixedLR, CosineLR, StepDropLR, WarmRestartsLR\n",
    "from optimizers.adafactor import Adafactor, AdafactorConfig\n",
    "from optimizers.combined import CombinedAdamWAdafactor, CombinedConfig"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "bT-dwVdwFO3b"
   },
   "source": [
    "def create_optimizer(variant_id, model, epochs=100, batch_size=128, lr=1e-3, wd=1e-4,projection_spec=\"none\", track_projection_metrics=False):\n",
    "    \"\"\"Create optimizer by variant ID.\"\"\"\n",
    "    total_steps = epochs * (len(x_train) // batch_size)\n",
    "    total_batches = len(x_train) // batch_size * epochs\n",
    "\n",
    "    variants = {\n",
    "        'A1': lambda: AdamL2(model.parameters(), OptimizerConfig(lr=lr, weight_decay=wd)),\n",
    "        'B1': lambda: AdamW(model.parameters(), AdamWConfig(lr=lr, weight_decay=wd), FixedLR(), total_steps),\n",
    "        'B2': lambda: AdamW(model.parameters(), AdamWConfig(lr=lr, weight_decay=wd), StepDropLR(), total_steps),\n",
    "        'B3': lambda: AdamW(model.parameters(), AdamWConfig(lr=lr, weight_decay=wd), CosineLR(), total_steps),\n",
    "        'B4': lambda: AdamW(model.parameters(), AdamWConfig(lr=lr, weight_decay=wd), WarmRestartsLR(), total_steps),\n",
    "        'B5': lambda: AdamW(model.parameters(), AdamWConfig(lr=lr, weight_decay=wd, normalized_decay=True, total_batches=total_batches), FixedLR(), total_steps),\n",
    "        'B6': lambda: AdamW(model.parameters(), AdamWConfig(lr=lr, weight_decay=wd, normalized_decay=True, total_batches=total_batches), StepDropLR(), total_steps),\n",
    "        'B7': lambda: AdamW(model.parameters(), AdamWConfig(lr=lr, weight_decay=wd, normalized_decay=True, total_batches=total_batches), CosineLR(), total_steps),\n",
    "        'B8': lambda: AdamW(model.parameters(), AdamWConfig(lr=lr, weight_decay=wd, normalized_decay=True, total_batches=total_batches), WarmRestartsLR(), total_steps),\n",
    "        'C1': lambda: Adafactor(\n",
    "            model.parameters(),\n",
    "            AdafactorConfig(\n",
    "            lr=lr, beta1=0.0, weight_decay=wd,\n",
    "            projection_spec=projection_spec,\n",
    "            track_projection_metrics=track_projection_metrics,\n",
    "            ),\n",
    "            FixedLR(),\n",
    "            total_steps\n",
    "        ),\n",
    "        'C2': lambda: Adafactor(\n",
    "            model.parameters(),\n",
    "            AdafactorConfig(\n",
    "            lr=lr, beta1=0.9, weight_decay=wd,\n",
    "            projection_spec=projection_spec,\n",
    "            track_projection_metrics=track_projection_metrics,\n",
    "            ),\n",
    "            FixedLR(),\n",
    "            total_steps\n",
    "        ),\n",
    "\n",
    "        'D1': lambda: CombinedAdamWAdafactor(\n",
    "            model.parameters(),\n",
    "            CombinedConfig(\n",
    "                lr=lr, beta1=0.0, weight_decay=wd,\n",
    "                normalized_decay=True, total_batches=total_batches\n",
    "            ),\n",
    "            CosineLR(),\n",
    "            total_steps\n",
    "        ),\n",
    "        'D2': lambda: CombinedAdamWAdafactor(\n",
    "            model.parameters(),\n",
    "            CombinedConfig(\n",
    "                lr=lr, beta1=0.0, weight_decay=wd,\n",
    "                normalized_decay=True, total_batches=total_batches\n",
    "            ),\n",
    "            WarmRestartsLR(),\n",
    "            total_steps\n",
    "        ),\n",
    "        'D3': lambda: CombinedAdamWAdafactor(\n",
    "            model.parameters(),\n",
    "            CombinedConfig(\n",
    "                lr=lr, beta1=0.9, weight_decay=wd,\n",
    "                normalized_decay=True, total_batches=total_batches\n",
    "            ),\n",
    "            WarmRestartsLR(),\n",
    "            total_steps\n",
    "        ),\n",
    "        'E1': lambda: Adafactor(model.parameters(), AdafactorConfig(lr=lr, beta1=0.0, weight_decay=wd, projection_spec='l2_ball:scale=param_rms,base=1,mult=1e-8', track_projection_metrics=track_projection_metrics), FixedLR(), total_steps),\n",
    "        'E2': lambda: Adafactor(model.parameters(), AdafactorConfig(lr=lr, beta1=0.9, weight_decay=wd, projection_spec='l2_ball:scale=param_rms,base=1,mult=1e-8', track_projection_metrics=track_projection_metrics), FixedLR(), total_steps),\n",
    "        'E3': lambda: Adafactor(model.parameters(), AdafactorConfig(lr=lr, beta1=0.0, weight_decay=wd, projection_spec='box:l=0,u=1e-9', track_projection_metrics=track_projection_metrics), FixedLR(), total_steps),\n",
    "        'E4': lambda: Adafactor(model.parameters(), AdafactorConfig(lr=lr, beta1=0.9, weight_decay=wd, projection_spec='box:l=0,u=1e-9', track_projection_metrics=track_projection_metrics), FixedLR(), total_steps),\n",
    "    }\n",
    "    return variants[variant_id]()\n",
    "\n",
    "VARIANT_NAMES = {\n",
    "    'A1': 'Adam+L2', 'B1': 'AdamW Fixed', 'B2': 'AdamW StepDrop', 'B3': 'AdamW Cosine',\n",
    "    'B4': 'AdamW WarmRestarts', 'B5': 'AdamW Fixed+Norm', 'B6': 'AdamW StepDrop+Norm',\n",
    "    'B7': 'AdamW Cosine+Norm', 'B8': 'AdamW WarmRestarts+Norm',\n",
    "    'C1': 'Adafactor (\u03b21=0)', 'C2': 'Adafactor (\u03b21=0.9)',\n",
    "    'D1': 'Adafactor+Norm Cosine (\u03b21=0)', 'D2': 'Adafactor+Norm WarmRestarts (\u03b21=0)', 'D3': 'Adafactor+Norm WarmRestarts (\u03b21=0.9)',\n",
    "    'E1': 'Adafactor+L2Ball (\u03b21=0)', 'E2': 'Adafactor+L2Ball (\u03b21=0.9)', 'E3': 'Adafactor+Box (\u03b21=0)', 'E4': 'Adafactor+Box (\u03b21=0.9)'\n",
    "}"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "8YZFi2Z9FO3d"
   },
   "source": [
    "def train_with_custom_optimizer(model, criterion, train_loader, optimizer, epochs=100):\n",
    "    \"\"\"Training loop using custom optimizer with oracle.\"\"\"\n",
    "    losses = []\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0\n",
    "        for batch in train_loader:\n",
    "            x = batch[0]\n",
    "            loss, grads, _ = autoencoder_oracle(model, criterion, x, calc_hessian=False)\n",
    "            for param, grad in zip(model.parameters(), grads):\n",
    "                param.grad = grad\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            epoch_loss += loss.item()\n",
    "        avg_loss = epoch_loss / len(train_loader)\n",
    "        losses.append(avg_loss)\n",
    "        if (epoch + 1) % 20 == 0 or epoch == 0:\n",
    "            print(f'Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.6f}')\n",
    "    return losses"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QryBNy_5FO3g"
   },
   "source": [
    "## Grid Search for Heatmaps"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "j9f_s8g9FO3h"
   },
   "source": [
    "from utils.logging import TrainingLog\n",
    "from utils.experiment import GridSearchResult, OptimizerExperiment\n",
    "\n",
    "# Hyperparameter grid\n",
    "LR_MULTS = [1/1024, 1/512, 1/256, 1/128, 1/64, 1/32, 1/16, 1/8, 1/4, 1/2, 1, 2]\n",
    "WD_MULTS = [0, 1/32, 1/16, 1/8, 1/4, 1/2, 1, 2, 4, 8, 16, 32]\n",
    "BASE_LR = 1e-3\n",
    "BASE_WD = 1e-3"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "FOLuojCQFO3h"
   },
   "source": [
    "def run_single_config(variant_id, lr, wd, epochs, batch_size, test_loader=None, log_every=10):\n",
    "    \"\"\"Train one config and return detailed log with test evaluation.\"\"\"\n",
    "    import time\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    model = Autoencoder().to(device)\n",
    "    opt = create_optimizer(variant_id, model, epochs=epochs, batch_size=batch_size, lr=lr, wd=wd)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    x_train_dev = x_train.to(device)\n",
    "    loader = DataLoader(TensorDataset(x_train_dev), batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    if test_loader:\n",
    "        x_test_dev = test_loader.dataset.tensors[0].to(device)\n",
    "        test_loader = DataLoader(TensorDataset(x_test_dev), batch_size=batch_size)\n",
    "\n",
    "    log = TrainingLog()\n",
    "    start_time = time.time()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        epoch_start = time.time()\n",
    "        epoch_loss = 0\n",
    "        for batch in loader:\n",
    "            x = batch[0]\n",
    "            model.zero_grad()\n",
    "            output = model(x)\n",
    "            loss = criterion(output, x)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        train_loss = epoch_loss / len(loader)\n",
    "\n",
    "        # Evaluate on test set\n",
    "        test_loss = train_loss\n",
    "        if test_loader:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                test_loss = sum(criterion(model(b[0]), b[0]).item() for b in test_loader) / len(test_loader)\n",
    "            model.train()\n",
    "\n",
    "        log.log_epoch(train_loss, test_loss, time.time() - epoch_start)\n",
    "\n",
    "    log.total_runtime = time.time() - start_time\n",
    "    return model, log"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "-vcxyni8FO3h"
   },
   "source": [
    "def run_grid_search(variant_id, epochs=100, batch_size=128, save_every=1):\n",
    "    \"\"\"Run grid search with checkpointing and resume support.\"\"\"\n",
    "    filename = f'{variant_id}_{VARIANT_NAMES[variant_id].replace(\" \", \"_\")}.json'\n",
    "    filepath = RESULTS_DIR + filename\n",
    "\n",
    "    # Try to resume from existing file\n",
    "    try:\n",
    "        experiment = OptimizerExperiment.load(filepath)\n",
    "        done = {(r.lr_multiplier, r.wd_multiplier) for r in experiment.results}\n",
    "        print(f'Resuming {variant_id}: {len(done)} configs already done')\n",
    "    except:\n",
    "        experiment = OptimizerExperiment(\n",
    "            optimizer_id=variant_id,\n",
    "            optimizer_name=VARIANT_NAMES[variant_id],\n",
    "            config={'epochs': epochs, 'batch_size': batch_size, 'base_lr': BASE_LR, 'base_wd': BASE_WD}\n",
    "        )\n",
    "        done = set()\n",
    "\n",
    "    test_loader = DataLoader(TensorDataset(x_test), batch_size=batch_size, shuffle=False)\n",
    "    total = len(LR_MULTS) * len(WD_MULTS)\n",
    "\n",
    "    for lr_mult in LR_MULTS:\n",
    "        for wd_mult in WD_MULTS:\n",
    "            if (lr_mult, wd_mult) in done:\n",
    "                continue\n",
    "\n",
    "            lr = BASE_LR * lr_mult\n",
    "            wd = BASE_WD * wd_mult\n",
    "\n",
    "            model, log = run_single_config(variant_id, lr, wd, epochs, batch_size, test_loader)\n",
    "\n",
    "            experiment.results.append(GridSearchResult(\n",
    "                lr_multiplier=lr_mult, wd_multiplier=wd_mult,\n",
    "                learning_rate=lr, weight_decay=wd,\n",
    "                final_train_loss=log.epoch_train_losses[-1],\n",
    "                final_test_loss=log.epoch_test_losses[-1],\n",
    "                best_test_loss=log.best_test_loss,\n",
    "                best_epoch=log.best_epoch,\n",
    "            ))\n",
    "\n",
    "            n = len(experiment.results)\n",
    "            print(f'\\r{variant_id}: {n}/{total} | lr={lr:.2e}, wd={wd:.2e}, test={log.epoch_test_losses[-1]:.4f}', end='')\n",
    "\n",
    "            if n % save_every == 0:\n",
    "                experiment.save(filepath)\n",
    "\n",
    "    print()\n",
    "    experiment.save(filepath)\n",
    "    return experiment"
   ],
   "outputs": [],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "2ZhNsKc2FO3i"
   },
   "source": [
    "# === CONFIGURATION ===\n",
    "PHASE = 1  # 1 = screening (30 epochs), 2 = full training (100 epochs)\n",
    "\n",
    "GRID_EPOCHS = 30 if PHASE == 1 else 100\n",
    "\n",
    "# Phase 1: all optimizers for screening\n",
    "# Phase 2: only winners (edit after Phase 1)\n",
    "VARIANTS_TO_RUN = ['A1', 'B1', 'B2', 'B3', 'B4', 'B5', 'B6', 'B7', 'B8', 'C1', 'C2', 'D1', 'D2', 'D3']\n",
    "\n",
    "print(f'Phase {PHASE}: {GRID_EPOCHS} epochs, {len(VARIANTS_TO_RUN)} optimizers')\n",
    "print(f'Estimated time: ~{len(VARIANTS_TO_RUN) * 144 * GRID_EPOCHS / 50 * 6 / 60:.0f} hours')\n",
    "\n",
    "experiments = {}\n",
    "for v in VARIANTS_TO_RUN:\n",
    "    print(f'\\n=== {v}: {VARIANT_NAMES[v]} ===')\n",
    "    experiments[v] = run_grid_search(v, epochs=GRID_EPOCHS)\n",
    "    print(f'Saved to Drive: {v}')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "cNifh-yEFO3o"
   },
   "cell_type": "markdown",
   "source": [
    "## Adafactor Projection Test\n",
    "\n",
    "This is a sandbox experiment to test projecting Adafactor's **factored second-moment running averages**\n",
    "(`vr`, `vc`) onto different convex sets.\n",
    "\n",
    "We run a short training (1 epoch for now) for C1 and C2 and log similarity metrics between `vr/vc` **before** and **after** projection (saved by the optimizer in `state[\"vr_pre_proj\"]`, `state[\"vc_pre_proj\"]`).\n"
   ]
  },
  {
   "metadata": {
    "collapsed": true,
    "id": "HMqvNCjkFO3p",
    "outputId": "cc2023b5-851b-409a-f152-019cb2334201",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Num projection specs: 15\n",
      "  none\n",
      "  nonneg\n",
      "  box:l=0,u=1e6\n",
      "  box:l=0,u=1\n",
      "  simplex:s=1.0\n",
      "  l2_ball:scale=param_rms,base=1,mult=1e-12\n",
      "  l2_ball:scale=param_rms,base=1,mult=1e-10\n",
      "  l2_ball:scale=param_rms,base=1,mult=1e-08\n",
      "  l2_ball:scale=param_rms,base=1,mult=1e-06\n",
      "  l2_ball:scale=param_rms,base=1,mult=0.0001\n",
      "  box:l=0,u=1e-12\n",
      "  box:l=0,u=1e-11\n",
      "  box:l=0,u=1e-10\n",
      "  box:l=0,u=1e-09\n",
      "  box:l=0,u=1e-08\n"
     ]
    }
   ],
   "execution_count": 37,
   "source": [
    "# --- runtime sweep specs (multiple L2 scales) ---\n",
    "\n",
    "# Multipliers you want to sweep for adaptive L2-ball\n",
    "L2_MULTS = [1e-12, 1e-10, 1e-8, 1e-6, 1e-4]\n",
    "# For the box proj\n",
    "BOX_US = [1e-12, 1e-11, 1e-10, 1e-9, 1e-8]\n",
    "\n",
    "# Build projection specs programmatically (no hardcoding) it is hardcoded\n",
    "PROJECTION_SPECS = [\"none\"] #, \"nonneg\"] #, \"box:l=0,u=1e6\",\"box:l=0,u=1\", \"simplex:s=1.0\"]\n",
    "PROJECTION_SPECS += [f\"l2_ball:scale=param_rms,base=1,mult={m}\" for m in L2_MULTS]\n",
    "PROJECTION_SPECS += [f\"box:l=0,u={u}\" for u in BOX_US]\n",
    "# Alternative scale option:\n",
    "# PROJECTION_SPECS += [f\"l2_ball:scale=vec_rms,base=1000,mult={m}\" for m in L2_MULTS]\n",
    "\n",
    "print(\"Num projection specs:\", len(PROJECTION_SPECS))\n",
    "for s in PROJECTION_SPECS:\n",
    "    print(\" \", s)"
   ]
  },
  {
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G8vNOwGQFO3p",
    "outputId": "2e749b96-e367-4325-e0aa-a197bcff30f0"
   },
   "cell_type": "code",
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Running spec='none' beta1=0.0 ...\n",
      "Running spec='none' beta1=0.9 ...\n",
      "Running spec='nonneg' beta1=0.0 ...\n",
      "Running spec='nonneg' beta1=0.9 ...\n",
      "Running spec='box:l=0,u=1e6' beta1=0.0 ...\n",
      "Running spec='box:l=0,u=1e6' beta1=0.9 ...\n",
      "Running spec='box:l=0,u=1' beta1=0.0 ...\n",
      "Running spec='box:l=0,u=1' beta1=0.9 ...\n",
      "Running spec='simplex:s=1.0' beta1=0.0 ...\n",
      "Running spec='simplex:s=1.0' beta1=0.9 ...\n",
      "Running spec='l2_ball:scale=param_rms,base=1,mult=1e-12' beta1=0.0 ...\n",
      "Running spec='l2_ball:scale=param_rms,base=1,mult=1e-12' beta1=0.9 ...\n",
      "Running spec='l2_ball:scale=param_rms,base=1,mult=1e-10' beta1=0.0 ...\n",
      "Running spec='l2_ball:scale=param_rms,base=1,mult=1e-10' beta1=0.9 ...\n",
      "Running spec='l2_ball:scale=param_rms,base=1,mult=1e-08' beta1=0.0 ...\n",
      "Running spec='l2_ball:scale=param_rms,base=1,mult=1e-08' beta1=0.9 ...\n",
      "Running spec='l2_ball:scale=param_rms,base=1,mult=1e-06' beta1=0.0 ...\n",
      "Running spec='l2_ball:scale=param_rms,base=1,mult=1e-06' beta1=0.9 ...\n",
      "Running spec='l2_ball:scale=param_rms,base=1,mult=0.0001' beta1=0.0 ...\n",
      "Running spec='l2_ball:scale=param_rms,base=1,mult=0.0001' beta1=0.9 ...\n",
      "Running spec='box:l=0,u=1e-12' beta1=0.0 ...\n",
      "Running spec='box:l=0,u=1e-12' beta1=0.9 ...\n",
      "Running spec='box:l=0,u=1e-11' beta1=0.0 ...\n",
      "Running spec='box:l=0,u=1e-11' beta1=0.9 ...\n",
      "Running spec='box:l=0,u=1e-10' beta1=0.0 ...\n",
      "Running spec='box:l=0,u=1e-10' beta1=0.9 ...\n",
      "Running spec='box:l=0,u=1e-09' beta1=0.0 ...\n",
      "Running spec='box:l=0,u=1e-09' beta1=0.9 ...\n",
      "Running spec='box:l=0,u=1e-08' beta1=0.0 ...\n",
      "Running spec='box:l=0,u=1e-08' beta1=0.9 ...\n",
      "Done. total runs: 30 elapsed_sec: 108.3601565361023\n",
      "Saved: results/optimizer_projection_test.json\n"
     ]
    }
   ],
   "execution_count": 38,
   "source": [
    "# =========================\n",
    "# Adafactor Projection Test: RUN (replace this whole cell)\n",
    "# =========================\n",
    "import os, json, time\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# --- do NOT change blocks above; define locally only if missing ---\n",
    "if \"device\" not in globals():\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"Using device:\", device)\n",
    "\n",
    "RESULTS_DIR = \"results/\"\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "def _norm(x: torch.Tensor) -> float:\n",
    "    return float(x.norm().item())\n",
    "\n",
    "def _maybe(x: torch.Tensor):\n",
    "    return x.detach() if torch.is_tensor(x) else x\n",
    "\n",
    "# knobs for the projection experiment\n",
    "SEED = 0\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "EPOCHS_PROJ = 1\n",
    "BATCH_SIZE_PROJ = 128\n",
    "MAX_BATCHES = 200\n",
    "LR_PROJ = 1e-3\n",
    "WD_PROJ = 1e-4\n",
    "\n",
    "# total_steps (only used if your Adafactor takes schedulers; define here anyway)\n",
    "steps_per_epoch = max(1, len(x_train) // BATCH_SIZE_PROJ) if \"x_train\" in globals() else MAX_BATCHES\n",
    "total_steps = EPOCHS_PROJ * steps_per_epoch\n",
    "\n",
    "# loader (expects x_train exists from earlier blocks)\n",
    "proj_loader = torch.utils.data.DataLoader(\n",
    "    torch.utils.data.TensorDataset(x_train),\n",
    "    batch_size=BATCH_SIZE_PROJ,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    ")\n",
    "\n",
    "def _cosine_sim(a: torch.Tensor, b: torch.Tensor, tol: float = 1e-12) -> float:\n",
    "    a = a.flatten()\n",
    "    b = b.flatten()\n",
    "    na = float(a.norm().item())\n",
    "    nb = float(b.norm().item())\n",
    "    if na < tol or nb < tol:\n",
    "        return float(\"nan\")\n",
    "    return float((a @ b).item() / (na * nb))\n",
    "\n",
    "def _rel_change(a: torch.Tensor, b: torch.Tensor, eps: float = 1e-12) -> float:\n",
    "    denom = a.norm().clamp_min(eps)\n",
    "    return float((b - a).norm() / denom)\n",
    "\n",
    "def run_one_projection_spec(projection_spec: str, beta1: float):\n",
    "    model = Autoencoder().to(device)\n",
    "    criterion = torch.nn.MSELoss()\n",
    "\n",
    "    # ---- Create optimizer WITHOUT touching blocks above ----\n",
    "    # If you already have create_optimizer above and want to use it, use the first option.\n",
    "    if \"create_optimizer\" in globals():\n",
    "        opt = create_optimizer(\n",
    "            \"C1\" if beta1 == 0.0 else \"C2\",\n",
    "            model,\n",
    "            epochs=EPOCHS_PROJ,\n",
    "            batch_size=BATCH_SIZE_PROJ,\n",
    "            lr=LR_PROJ,\n",
    "            wd=WD_PROJ,\n",
    "            projection_spec=projection_spec,\n",
    "            track_projection_metrics=True,\n",
    "        )\n",
    "    else:\n",
    "        # fallback: construct Adafactor directly (requires Adafactor/AdafactorConfig/WarmRestartsLR already imported above)\n",
    "        opt = Adafactor(\n",
    "            model.parameters(),\n",
    "            AdafactorConfig(\n",
    "                lr=LR_PROJ,\n",
    "                weight_decay=WD_PROJ,\n",
    "                beta1=beta1,\n",
    "                projection_spec=projection_spec,\n",
    "                track_projection_metrics=True,\n",
    "            ),\n",
    "            WarmRestartsLR(),\n",
    "            total_steps,\n",
    "        )\n",
    "\n",
    "    step_metrics = []\n",
    "    losses = []\n",
    "\n",
    "    it = iter(proj_loader)\n",
    "    for step in range(MAX_BATCHES):\n",
    "        try:\n",
    "            batch = next(it)[0].to(device)\n",
    "        except StopIteration:\n",
    "            it = iter(proj_loader)\n",
    "            batch = next(it)[0].to(device)\n",
    "\n",
    "        loss, grads, _ = autoencoder_oracle(model, criterion, batch, calc_hessian=False)\n",
    "\n",
    "        for p, g in zip(model.parameters(), grads):\n",
    "            p.grad = g\n",
    "\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "\n",
    "        losses.append(float(loss.item()))\n",
    "\n",
    "        # pull projection stats from optimizer state (if you stored vr_pre_proj/vc_pre_proj)\n",
    "        vr_rel, vc_rel, vr_cos, vc_cos = [], [], [], []\n",
    "        vr_pre_norm, vr_post_norm, vc_pre_norm, vc_post_norm = [], [], [], []\n",
    "        matched = 0\n",
    "\n",
    "        state_dict = getattr(opt, \"state\", None)\n",
    "\n",
    "        if isinstance(state_dict, dict):\n",
    "            for st in state_dict.values():\n",
    "                if not isinstance(st, dict):\n",
    "                    continue\n",
    "\n",
    "                if all(k in st for k in (\"vr\", \"vc\", \"vr_pre_proj\", \"vc_pre_proj\")):\n",
    "                    matched += 1\n",
    "                    vr_pre, vr_post = _maybe(st[\"vr_pre_proj\"]), _maybe(st[\"vr\"])\n",
    "                    vc_pre, vc_post = _maybe(st[\"vc_pre_proj\"]), _maybe(st[\"vc\"])\n",
    "\n",
    "                    # norms\n",
    "                    vr_pre_norm.append(_norm(vr_pre))\n",
    "                    vr_post_norm.append(_norm(vr_post))\n",
    "                    vc_pre_norm.append(_norm(vc_pre))\n",
    "                    vc_post_norm.append(_norm(vc_post))\n",
    "\n",
    "                    # changes\n",
    "                    vr_rel.append(_rel_change(vr_pre, vr_post))\n",
    "                    vc_rel.append(_rel_change(vc_pre, vc_post))\n",
    "                    vr_cos.append(_cosine_sim(vr_pre, vr_post))  # safe cosine\n",
    "                    vc_cos.append(_cosine_sim(vc_pre, vc_post))\n",
    "\n",
    "\n",
    "        step_metrics.append({\n",
    "            \"step\": step,\n",
    "            \"loss\": float(loss.item()),\n",
    "            \"matched_states\": matched,\n",
    "\n",
    "            \"vr_pre_norm\": float(np.nanmean(vr_pre_norm)) if vr_pre_norm else float(\"nan\"),\n",
    "            \"vr_post_norm\": float(np.nanmean(vr_post_norm)) if vr_post_norm else float(\"nan\"),\n",
    "            \"vc_pre_norm\": float(np.nanmean(vc_pre_norm)) if vc_pre_norm else float(\"nan\"),\n",
    "            \"vc_post_norm\": float(np.nanmean(vc_post_norm)) if vc_post_norm else float(\"nan\"),\n",
    "\n",
    "            \"vr_rel\": float(np.mean(vr_rel)) if vr_rel else float(\"nan\"),\n",
    "            \"vc_rel\": float(np.mean(vc_rel)) if vc_rel else float(\"nan\"),\n",
    "            \"vr_cos\": float(np.mean(vr_cos)) if vr_cos else float(\"nan\"),\n",
    "            \"vc_cos\": float(np.mean(vc_cos)) if vc_cos else float(\"nan\"),\n",
    "        })\n",
    "\n",
    "    return {\n",
    "        \"projection_spec\": projection_spec,\n",
    "        \"beta1\": beta1,\n",
    "        \"loss_start\": losses[0] if losses else None,\n",
    "        \"loss_end\": losses[-1] if losses else None,\n",
    "        \"metrics\": step_metrics,\n",
    "    }\n",
    "\n",
    "# --- run sweep ---\n",
    "all_runs = []\n",
    "t0 = time.time()\n",
    "\n",
    "for spec in PROJECTION_SPECS:\n",
    "    for beta1 in (0.0, 0.9):\n",
    "        print(f\"Running spec='{spec}' beta1={beta1} ...\")\n",
    "        all_runs.append(run_one_projection_spec(spec, beta1))\n",
    "\n",
    "print(\"Done. total runs:\", len(all_runs), \"elapsed_sec:\", time.time() - t0)\n",
    "\n",
    "out_payload = {\n",
    "    \"meta\": {\n",
    "        \"seed\": SEED,\n",
    "        \"lr\": LR_PROJ,\n",
    "        \"weight_decay\": WD_PROJ,\n",
    "        \"max_batches\": MAX_BATCHES,\n",
    "        \"batch_size\": BATCH_SIZE_PROJ,\n",
    "        \"projection_specs\": list(PROJECTION_SPECS),\n",
    "    },\n",
    "    \"runs\": all_runs,\n",
    "}\n",
    "\n",
    "out_path = os.path.join(RESULTS_DIR, \"optimizer_projection_test.json\")\n",
    "with open(out_path, \"w\") as f:\n",
    "    json.dump(out_payload, f, indent=2)\n",
    "\n",
    "print(\"Saved:\", out_path)\n"
   ]
  },
  {
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "collapsed": true,
    "id": "_imbL59LFO3r",
    "outputId": "ecf471a6-8028-4180-91e3-06fd2b10d1ac"
   },
   "cell_type": "code",
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "    beta1                                  projection  steps   vr_rel_mean  \\\n",
       "0     0.0                                 box:l=0,u=1    200  0.000000e+00   \n",
       "1     0.0                             box:l=0,u=1e-08    200  0.000000e+00   \n",
       "2     0.0                             box:l=0,u=1e-09    200  7.521221e-03   \n",
       "3     0.0                             box:l=0,u=1e-10    200  6.399449e-02   \n",
       "4     0.0                             box:l=0,u=1e-11    200  2.815176e-01   \n",
       "5     0.0                             box:l=0,u=1e-12    200  6.997876e-01   \n",
       "6     0.0                               box:l=0,u=1e6    200  0.000000e+00   \n",
       "7     0.0  l2_ball:scale=param_rms,base=1,mult=0.0001    200  0.000000e+00   \n",
       "8     0.0   l2_ball:scale=param_rms,base=1,mult=1e-06    200  0.000000e+00   \n",
       "9     0.0   l2_ball:scale=param_rms,base=1,mult=1e-08    200  6.818761e-02   \n",
       "10    0.0   l2_ball:scale=param_rms,base=1,mult=1e-10    200  7.008160e-01   \n",
       "11    0.0   l2_ball:scale=param_rms,base=1,mult=1e-12    200  9.950806e-01   \n",
       "12    0.0                                        none    200  0.000000e+00   \n",
       "13    0.0                                      nonneg    200  0.000000e+00   \n",
       "14    0.0                               simplex:s=1.0    200  1.070207e+06   \n",
       "15    0.9                                 box:l=0,u=1    200  0.000000e+00   \n",
       "16    0.9                             box:l=0,u=1e-08    200  0.000000e+00   \n",
       "17    0.9                             box:l=0,u=1e-09    200  6.924941e-03   \n",
       "18    0.9                             box:l=0,u=1e-10    200  4.304976e-02   \n",
       "19    0.9                             box:l=0,u=1e-11    200  2.946465e-01   \n",
       "20    0.9                             box:l=0,u=1e-12    200  6.485058e-01   \n",
       "21    0.9                               box:l=0,u=1e6    200  0.000000e+00   \n",
       "22    0.9  l2_ball:scale=param_rms,base=1,mult=0.0001    200  0.000000e+00   \n",
       "23    0.9   l2_ball:scale=param_rms,base=1,mult=1e-06    200  0.000000e+00   \n",
       "24    0.9   l2_ball:scale=param_rms,base=1,mult=1e-08    200  6.108647e-02   \n",
       "25    0.9   l2_ball:scale=param_rms,base=1,mult=1e-10    200  6.611323e-01   \n",
       "26    0.9   l2_ball:scale=param_rms,base=1,mult=1e-12    200  9.950516e-01   \n",
       "27    0.9                                        none    200  0.000000e+00   \n",
       "28    0.9                                      nonneg    200  0.000000e+00   \n",
       "29    0.9                               simplex:s=1.0    200  3.292186e+06   \n",
       "\n",
       "    vr_rel_p95    vr_rel_max  vr_active_%   vc_rel_mean  vc_rel_p95  \\\n",
       "0     0.000000  0.000000e+00          0.0  0.000000e+00    0.000000   \n",
       "1     0.000000  0.000000e+00          0.0  0.000000e+00    0.000000   \n",
       "2     0.022620  9.461258e-02        100.0  3.589418e-03    0.010555   \n",
       "3     0.195711  4.513849e-01        100.0  5.940768e-02    0.185531   \n",
       "4     0.532220  8.070680e-01        100.0  2.697438e-01    0.533470   \n",
       "5     0.886791  9.799144e-01        100.0  6.995329e-01    0.854887   \n",
       "6     0.000000  0.000000e+00          0.0  0.000000e+00    0.000000   \n",
       "7     0.000000  0.000000e+00          0.0  0.000000e+00    0.000000   \n",
       "8     0.000000  0.000000e+00          0.0  0.000000e+00    0.000000   \n",
       "9     0.168765  3.483125e-01         99.5  4.300517e-02    0.121290   \n",
       "10    0.887915  9.805832e-01        100.0  7.198050e-01    0.904506   \n",
       "11    0.999054  9.997859e-01        100.0  9.955513e-01    0.999178   \n",
       "12    0.000000  0.000000e+00          0.0  0.000000e+00    0.000000   \n",
       "13    0.000000  0.000000e+00          0.0  0.000000e+00    0.000000   \n",
       "14    0.172943  2.140414e+08        100.0  5.701892e+05    0.172943   \n",
       "15    0.000000  0.000000e+00          0.0  0.000000e+00    0.000000   \n",
       "16    0.000000  0.000000e+00          0.0  0.000000e+00    0.000000   \n",
       "17    0.020446  8.876699e-02        100.0  5.308228e-03    0.015896   \n",
       "18    0.130922  3.779296e-01        100.0  4.761606e-02    0.142443   \n",
       "19    0.540660  7.716413e-01        100.0  3.260538e-01    0.581515   \n",
       "20    0.837679  9.677670e-01        100.0  6.458738e-01    0.806150   \n",
       "21    0.000000  0.000000e+00          0.0  0.000000e+00    0.000000   \n",
       "22    0.000000  0.000000e+00          0.0  0.000000e+00    0.000000   \n",
       "23    0.000000  0.000000e+00          0.0  0.000000e+00    0.000000   \n",
       "24    0.150926  2.874323e-01         99.5  4.021644e-02    0.107700   \n",
       "25    0.855118  9.743547e-01        100.0  6.965264e-01    0.891396   \n",
       "26    0.999119  9.997933e-01        100.0  9.962287e-01    0.999316   \n",
       "27    0.000000  0.000000e+00          0.0  0.000000e+00    0.000000   \n",
       "28    0.000000  0.000000e+00          0.0  0.000000e+00    0.000000   \n",
       "29    0.172943  6.584373e+08        100.0  1.424503e+06    0.172943   \n",
       "\n",
       "      vc_rel_max  ...  vr_cos_min  vc_cos_mean  vc_cos_min  loss_start  \\\n",
       "0   0.000000e+00  ...    1.000000     1.000000    1.000000    0.167440   \n",
       "1   0.000000e+00  ...    1.000000     1.000000    1.000000    0.173084   \n",
       "2   6.372707e-02  ...    0.977733     0.999932    0.994600    0.170057   \n",
       "3   4.864655e-01  ...    0.884987     0.996655    0.899546    0.170880   \n",
       "4   7.645293e-01  ...    0.763494     0.961927    0.779905    0.170326   \n",
       "5   9.585844e-01  ...    0.618875     0.794344    0.659073    0.169236   \n",
       "6   0.000000e+00  ...    1.000000     1.000000    1.000000    0.167099   \n",
       "7   0.000000e+00  ...    1.000000     1.000000    1.000000    0.170543   \n",
       "8   0.000000e+00  ...    1.000000     1.000000    1.000000    0.172142   \n",
       "9   2.916704e-01  ...    1.000000     1.000000    1.000000    0.168850   \n",
       "10  9.824745e-01  ...    1.000000     1.000000    1.000000    0.172769   \n",
       "11  9.997962e-01  ...         NaN          NaN         NaN    0.170833   \n",
       "12  0.000000e+00  ...    1.000000     1.000000    1.000000    0.170316   \n",
       "13  0.000000e+00  ...    1.000000     1.000000    1.000000    0.168599   \n",
       "14  1.140378e+08  ...    0.513778     0.997932    0.586487    0.167229   \n",
       "15  0.000000e+00  ...    1.000000     1.000000    1.000000    0.176031   \n",
       "16  0.000000e+00  ...    1.000000     1.000000    1.000000    0.167721   \n",
       "17  9.265922e-02  ...    0.985730     0.999868    0.989832    0.171650   \n",
       "18  4.188044e-01  ...    0.924183     0.997933    0.931009    0.168656   \n",
       "19  7.523942e-01  ...    0.789544     0.943178    0.765933    0.171510   \n",
       "20  9.170313e-01  ...    0.624180     0.822165    0.677949    0.171535   \n",
       "21  0.000000e+00  ...    1.000000     1.000000    1.000000    0.170728   \n",
       "22  0.000000e+00  ...    1.000000     1.000000    1.000000    0.170139   \n",
       "23  0.000000e+00  ...    1.000000     1.000000    1.000000    0.165409   \n",
       "24  2.599851e-01  ...    1.000000     1.000000    1.000000    0.171407   \n",
       "25  9.801586e-01  ...    1.000000     1.000000    1.000000    0.177750   \n",
       "26  9.998362e-01  ...         NaN          NaN         NaN    0.168976   \n",
       "27  0.000000e+00  ...    1.000000     1.000000    1.000000    0.170325   \n",
       "28  0.000000e+00  ...    1.000000     1.000000    1.000000    0.172126   \n",
       "29  2.849005e+08  ...    0.542510     0.997835    0.567103    0.169964   \n",
       "\n",
       "    loss_end  matched_states_mean  vr_pre_norm_mean  vr_post_norm_mean  \\\n",
       "0   0.169205                  6.0      2.534424e-09       2.534424e-09   \n",
       "1   0.165977                  6.0      2.161803e-09       2.161803e-09   \n",
       "2   0.171322                  6.0      1.927867e-09       1.912921e-09   \n",
       "3   0.173588                  6.0      6.245025e-10       5.695546e-10   \n",
       "4   0.172437                  6.0      1.394598e-10       8.335460e-11   \n",
       "5   0.165856                  6.0      1.360660e-10       1.089492e-11   \n",
       "6   0.168706                  6.0      2.524481e-09       2.524481e-09   \n",
       "7   0.167198                  6.0      2.734572e-09       2.734572e-09   \n",
       "8   0.170000                  6.0      2.234302e-09       2.234302e-09   \n",
       "9   0.172670                  6.0      5.470785e-10       4.789768e-10   \n",
       "10  0.174915                  6.0      1.159836e-10       6.312421e-12   \n",
       "11  0.165134                  6.0      1.293432e-10       6.365198e-14   \n",
       "12  0.168290                  6.0      1.817394e-09       1.817394e-09   \n",
       "13  0.169203                  6.0      1.603191e-09       1.603191e-09   \n",
       "14  0.168668                  6.0      9.712010e-02       1.022299e-01   \n",
       "15  0.172115                  6.0      2.608326e-09       2.608326e-09   \n",
       "16  0.170706                  6.0      1.907215e-09       1.907215e-09   \n",
       "17  0.174718                  6.0      1.906565e-09       1.883207e-09   \n",
       "18  0.168975                  6.0      5.854870e-10       5.578912e-10   \n",
       "19  0.170100                  6.0      1.602694e-10       8.302122e-11   \n",
       "20  0.173596                  6.0      1.190489e-10       1.030948e-11   \n",
       "21  0.175025                  6.0      2.117142e-09       2.117142e-09   \n",
       "22  0.172024                  6.0      2.613099e-09       2.613099e-09   \n",
       "23  0.170602                  6.0      1.974606e-09       1.974606e-09   \n",
       "24  0.173564                  6.0      4.726302e-10       4.134474e-10   \n",
       "25  0.170053                  6.0      1.467630e-10       6.314605e-12   \n",
       "26  0.169896                  6.0      1.065305e-10       6.396818e-14   \n",
       "27  0.168462                  6.0      2.045493e-09       2.045493e-09   \n",
       "28  0.171733                  6.0      1.496424e-09       1.496424e-09   \n",
       "29  0.165664                  6.0      9.712010e-02       1.022299e-01   \n",
       "\n",
       "    vc_pre_norm_mean  vc_post_norm_mean  \n",
       "0       1.585288e-09       1.585288e-09  \n",
       "1       1.259594e-09       1.259594e-09  \n",
       "2       1.307409e-09       1.302382e-09  \n",
       "3       4.007302e-10       3.672376e-10  \n",
       "4       9.859487e-11       6.377454e-11  \n",
       "5       8.887698e-11       1.016928e-11  \n",
       "6       1.903456e-09       1.903456e-09  \n",
       "7       1.734238e-09       1.734238e-09  \n",
       "8       1.482614e-09       1.482614e-09  \n",
       "9       5.033299e-10       4.680807e-10  \n",
       "10      7.536501e-11       6.316533e-12  \n",
       "11      7.867043e-11       6.365303e-14  \n",
       "12      1.262752e-09       1.262752e-09  \n",
       "13      1.129273e-09       1.129273e-09  \n",
       "14      9.712010e-02       1.022299e-01  \n",
       "15      1.693627e-09       1.693627e-09  \n",
       "16      1.269396e-09       1.269396e-09  \n",
       "17      1.360323e-09       1.351051e-09  \n",
       "18      3.726090e-10       3.509911e-10  \n",
       "19      1.124873e-10       5.722829e-11  \n",
       "20      7.737615e-11       9.495710e-12  \n",
       "21      1.429457e-09       1.429457e-09  \n",
       "22      1.595298e-09       1.595298e-09  \n",
       "23      1.339134e-09       1.339134e-09  \n",
       "24      4.005213e-10       3.667011e-10  \n",
       "25      8.970747e-11       6.323013e-12  \n",
       "26      7.037172e-11       6.396818e-14  \n",
       "27      1.564998e-09       1.564998e-09  \n",
       "28      8.889774e-10       8.889774e-10  \n",
       "29      9.712010e-02       1.022299e-01  \n",
       "\n",
       "[30 rows x 22 columns]"
      ],
      "text/html": [
       "\n",
       "  <div id=\"df-6bb121b6-f63f-49d1-bf1b-ab90d65bc7dc\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>beta1</th>\n",
       "      <th>projection</th>\n",
       "      <th>steps</th>\n",
       "      <th>vr_rel_mean</th>\n",
       "      <th>vr_rel_p95</th>\n",
       "      <th>vr_rel_max</th>\n",
       "      <th>vr_active_%</th>\n",
       "      <th>vc_rel_mean</th>\n",
       "      <th>vc_rel_p95</th>\n",
       "      <th>vc_rel_max</th>\n",
       "      <th>...</th>\n",
       "      <th>vr_cos_min</th>\n",
       "      <th>vc_cos_mean</th>\n",
       "      <th>vc_cos_min</th>\n",
       "      <th>loss_start</th>\n",
       "      <th>loss_end</th>\n",
       "      <th>matched_states_mean</th>\n",
       "      <th>vr_pre_norm_mean</th>\n",
       "      <th>vr_post_norm_mean</th>\n",
       "      <th>vc_pre_norm_mean</th>\n",
       "      <th>vc_post_norm_mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>box:l=0,u=1</td>\n",
       "      <td>200</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.167440</td>\n",
       "      <td>0.169205</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.534424e-09</td>\n",
       "      <td>2.534424e-09</td>\n",
       "      <td>1.585288e-09</td>\n",
       "      <td>1.585288e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>box:l=0,u=1e-08</td>\n",
       "      <td>200</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.173084</td>\n",
       "      <td>0.165977</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.161803e-09</td>\n",
       "      <td>2.161803e-09</td>\n",
       "      <td>1.259594e-09</td>\n",
       "      <td>1.259594e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>box:l=0,u=1e-09</td>\n",
       "      <td>200</td>\n",
       "      <td>7.521221e-03</td>\n",
       "      <td>0.022620</td>\n",
       "      <td>9.461258e-02</td>\n",
       "      <td>100.0</td>\n",
       "      <td>3.589418e-03</td>\n",
       "      <td>0.010555</td>\n",
       "      <td>6.372707e-02</td>\n",
       "      <td>...</td>\n",
       "      <td>0.977733</td>\n",
       "      <td>0.999932</td>\n",
       "      <td>0.994600</td>\n",
       "      <td>0.170057</td>\n",
       "      <td>0.171322</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.927867e-09</td>\n",
       "      <td>1.912921e-09</td>\n",
       "      <td>1.307409e-09</td>\n",
       "      <td>1.302382e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>box:l=0,u=1e-10</td>\n",
       "      <td>200</td>\n",
       "      <td>6.399449e-02</td>\n",
       "      <td>0.195711</td>\n",
       "      <td>4.513849e-01</td>\n",
       "      <td>100.0</td>\n",
       "      <td>5.940768e-02</td>\n",
       "      <td>0.185531</td>\n",
       "      <td>4.864655e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>0.884987</td>\n",
       "      <td>0.996655</td>\n",
       "      <td>0.899546</td>\n",
       "      <td>0.170880</td>\n",
       "      <td>0.173588</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.245025e-10</td>\n",
       "      <td>5.695546e-10</td>\n",
       "      <td>4.007302e-10</td>\n",
       "      <td>3.672376e-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>box:l=0,u=1e-11</td>\n",
       "      <td>200</td>\n",
       "      <td>2.815176e-01</td>\n",
       "      <td>0.532220</td>\n",
       "      <td>8.070680e-01</td>\n",
       "      <td>100.0</td>\n",
       "      <td>2.697438e-01</td>\n",
       "      <td>0.533470</td>\n",
       "      <td>7.645293e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>0.763494</td>\n",
       "      <td>0.961927</td>\n",
       "      <td>0.779905</td>\n",
       "      <td>0.170326</td>\n",
       "      <td>0.172437</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.394598e-10</td>\n",
       "      <td>8.335460e-11</td>\n",
       "      <td>9.859487e-11</td>\n",
       "      <td>6.377454e-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0</td>\n",
       "      <td>box:l=0,u=1e-12</td>\n",
       "      <td>200</td>\n",
       "      <td>6.997876e-01</td>\n",
       "      <td>0.886791</td>\n",
       "      <td>9.799144e-01</td>\n",
       "      <td>100.0</td>\n",
       "      <td>6.995329e-01</td>\n",
       "      <td>0.854887</td>\n",
       "      <td>9.585844e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>0.618875</td>\n",
       "      <td>0.794344</td>\n",
       "      <td>0.659073</td>\n",
       "      <td>0.169236</td>\n",
       "      <td>0.165856</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.360660e-10</td>\n",
       "      <td>1.089492e-11</td>\n",
       "      <td>8.887698e-11</td>\n",
       "      <td>1.016928e-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.0</td>\n",
       "      <td>box:l=0,u=1e6</td>\n",
       "      <td>200</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.167099</td>\n",
       "      <td>0.168706</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.524481e-09</td>\n",
       "      <td>2.524481e-09</td>\n",
       "      <td>1.903456e-09</td>\n",
       "      <td>1.903456e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.0</td>\n",
       "      <td>l2_ball:scale=param_rms,base=1,mult=0.0001</td>\n",
       "      <td>200</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.170543</td>\n",
       "      <td>0.167198</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.734572e-09</td>\n",
       "      <td>2.734572e-09</td>\n",
       "      <td>1.734238e-09</td>\n",
       "      <td>1.734238e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.0</td>\n",
       "      <td>l2_ball:scale=param_rms,base=1,mult=1e-06</td>\n",
       "      <td>200</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.172142</td>\n",
       "      <td>0.170000</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.234302e-09</td>\n",
       "      <td>2.234302e-09</td>\n",
       "      <td>1.482614e-09</td>\n",
       "      <td>1.482614e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.0</td>\n",
       "      <td>l2_ball:scale=param_rms,base=1,mult=1e-08</td>\n",
       "      <td>200</td>\n",
       "      <td>6.818761e-02</td>\n",
       "      <td>0.168765</td>\n",
       "      <td>3.483125e-01</td>\n",
       "      <td>99.5</td>\n",
       "      <td>4.300517e-02</td>\n",
       "      <td>0.121290</td>\n",
       "      <td>2.916704e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.168850</td>\n",
       "      <td>0.172670</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.470785e-10</td>\n",
       "      <td>4.789768e-10</td>\n",
       "      <td>5.033299e-10</td>\n",
       "      <td>4.680807e-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.0</td>\n",
       "      <td>l2_ball:scale=param_rms,base=1,mult=1e-10</td>\n",
       "      <td>200</td>\n",
       "      <td>7.008160e-01</td>\n",
       "      <td>0.887915</td>\n",
       "      <td>9.805832e-01</td>\n",
       "      <td>100.0</td>\n",
       "      <td>7.198050e-01</td>\n",
       "      <td>0.904506</td>\n",
       "      <td>9.824745e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.172769</td>\n",
       "      <td>0.174915</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.159836e-10</td>\n",
       "      <td>6.312421e-12</td>\n",
       "      <td>7.536501e-11</td>\n",
       "      <td>6.316533e-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.0</td>\n",
       "      <td>l2_ball:scale=param_rms,base=1,mult=1e-12</td>\n",
       "      <td>200</td>\n",
       "      <td>9.950806e-01</td>\n",
       "      <td>0.999054</td>\n",
       "      <td>9.997859e-01</td>\n",
       "      <td>100.0</td>\n",
       "      <td>9.955513e-01</td>\n",
       "      <td>0.999178</td>\n",
       "      <td>9.997962e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.170833</td>\n",
       "      <td>0.165134</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.293432e-10</td>\n",
       "      <td>6.365198e-14</td>\n",
       "      <td>7.867043e-11</td>\n",
       "      <td>6.365303e-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.0</td>\n",
       "      <td>none</td>\n",
       "      <td>200</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.170316</td>\n",
       "      <td>0.168290</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.817394e-09</td>\n",
       "      <td>1.817394e-09</td>\n",
       "      <td>1.262752e-09</td>\n",
       "      <td>1.262752e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.0</td>\n",
       "      <td>nonneg</td>\n",
       "      <td>200</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.168599</td>\n",
       "      <td>0.169203</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.603191e-09</td>\n",
       "      <td>1.603191e-09</td>\n",
       "      <td>1.129273e-09</td>\n",
       "      <td>1.129273e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.0</td>\n",
       "      <td>simplex:s=1.0</td>\n",
       "      <td>200</td>\n",
       "      <td>1.070207e+06</td>\n",
       "      <td>0.172943</td>\n",
       "      <td>2.140414e+08</td>\n",
       "      <td>100.0</td>\n",
       "      <td>5.701892e+05</td>\n",
       "      <td>0.172943</td>\n",
       "      <td>1.140378e+08</td>\n",
       "      <td>...</td>\n",
       "      <td>0.513778</td>\n",
       "      <td>0.997932</td>\n",
       "      <td>0.586487</td>\n",
       "      <td>0.167229</td>\n",
       "      <td>0.168668</td>\n",
       "      <td>6.0</td>\n",
       "      <td>9.712010e-02</td>\n",
       "      <td>1.022299e-01</td>\n",
       "      <td>9.712010e-02</td>\n",
       "      <td>1.022299e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.9</td>\n",
       "      <td>box:l=0,u=1</td>\n",
       "      <td>200</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.176031</td>\n",
       "      <td>0.172115</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.608326e-09</td>\n",
       "      <td>2.608326e-09</td>\n",
       "      <td>1.693627e-09</td>\n",
       "      <td>1.693627e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.9</td>\n",
       "      <td>box:l=0,u=1e-08</td>\n",
       "      <td>200</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.167721</td>\n",
       "      <td>0.170706</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.907215e-09</td>\n",
       "      <td>1.907215e-09</td>\n",
       "      <td>1.269396e-09</td>\n",
       "      <td>1.269396e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.9</td>\n",
       "      <td>box:l=0,u=1e-09</td>\n",
       "      <td>200</td>\n",
       "      <td>6.924941e-03</td>\n",
       "      <td>0.020446</td>\n",
       "      <td>8.876699e-02</td>\n",
       "      <td>100.0</td>\n",
       "      <td>5.308228e-03</td>\n",
       "      <td>0.015896</td>\n",
       "      <td>9.265922e-02</td>\n",
       "      <td>...</td>\n",
       "      <td>0.985730</td>\n",
       "      <td>0.999868</td>\n",
       "      <td>0.989832</td>\n",
       "      <td>0.171650</td>\n",
       "      <td>0.174718</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.906565e-09</td>\n",
       "      <td>1.883207e-09</td>\n",
       "      <td>1.360323e-09</td>\n",
       "      <td>1.351051e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.9</td>\n",
       "      <td>box:l=0,u=1e-10</td>\n",
       "      <td>200</td>\n",
       "      <td>4.304976e-02</td>\n",
       "      <td>0.130922</td>\n",
       "      <td>3.779296e-01</td>\n",
       "      <td>100.0</td>\n",
       "      <td>4.761606e-02</td>\n",
       "      <td>0.142443</td>\n",
       "      <td>4.188044e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>0.924183</td>\n",
       "      <td>0.997933</td>\n",
       "      <td>0.931009</td>\n",
       "      <td>0.168656</td>\n",
       "      <td>0.168975</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.854870e-10</td>\n",
       "      <td>5.578912e-10</td>\n",
       "      <td>3.726090e-10</td>\n",
       "      <td>3.509911e-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.9</td>\n",
       "      <td>box:l=0,u=1e-11</td>\n",
       "      <td>200</td>\n",
       "      <td>2.946465e-01</td>\n",
       "      <td>0.540660</td>\n",
       "      <td>7.716413e-01</td>\n",
       "      <td>100.0</td>\n",
       "      <td>3.260538e-01</td>\n",
       "      <td>0.581515</td>\n",
       "      <td>7.523942e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>0.789544</td>\n",
       "      <td>0.943178</td>\n",
       "      <td>0.765933</td>\n",
       "      <td>0.171510</td>\n",
       "      <td>0.170100</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.602694e-10</td>\n",
       "      <td>8.302122e-11</td>\n",
       "      <td>1.124873e-10</td>\n",
       "      <td>5.722829e-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.9</td>\n",
       "      <td>box:l=0,u=1e-12</td>\n",
       "      <td>200</td>\n",
       "      <td>6.485058e-01</td>\n",
       "      <td>0.837679</td>\n",
       "      <td>9.677670e-01</td>\n",
       "      <td>100.0</td>\n",
       "      <td>6.458738e-01</td>\n",
       "      <td>0.806150</td>\n",
       "      <td>9.170313e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>0.624180</td>\n",
       "      <td>0.822165</td>\n",
       "      <td>0.677949</td>\n",
       "      <td>0.171535</td>\n",
       "      <td>0.173596</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.190489e-10</td>\n",
       "      <td>1.030948e-11</td>\n",
       "      <td>7.737615e-11</td>\n",
       "      <td>9.495710e-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.9</td>\n",
       "      <td>box:l=0,u=1e6</td>\n",
       "      <td>200</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.170728</td>\n",
       "      <td>0.175025</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.117142e-09</td>\n",
       "      <td>2.117142e-09</td>\n",
       "      <td>1.429457e-09</td>\n",
       "      <td>1.429457e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.9</td>\n",
       "      <td>l2_ball:scale=param_rms,base=1,mult=0.0001</td>\n",
       "      <td>200</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.170139</td>\n",
       "      <td>0.172024</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.613099e-09</td>\n",
       "      <td>2.613099e-09</td>\n",
       "      <td>1.595298e-09</td>\n",
       "      <td>1.595298e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.9</td>\n",
       "      <td>l2_ball:scale=param_rms,base=1,mult=1e-06</td>\n",
       "      <td>200</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.165409</td>\n",
       "      <td>0.170602</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.974606e-09</td>\n",
       "      <td>1.974606e-09</td>\n",
       "      <td>1.339134e-09</td>\n",
       "      <td>1.339134e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.9</td>\n",
       "      <td>l2_ball:scale=param_rms,base=1,mult=1e-08</td>\n",
       "      <td>200</td>\n",
       "      <td>6.108647e-02</td>\n",
       "      <td>0.150926</td>\n",
       "      <td>2.874323e-01</td>\n",
       "      <td>99.5</td>\n",
       "      <td>4.021644e-02</td>\n",
       "      <td>0.107700</td>\n",
       "      <td>2.599851e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.171407</td>\n",
       "      <td>0.173564</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.726302e-10</td>\n",
       "      <td>4.134474e-10</td>\n",
       "      <td>4.005213e-10</td>\n",
       "      <td>3.667011e-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.9</td>\n",
       "      <td>l2_ball:scale=param_rms,base=1,mult=1e-10</td>\n",
       "      <td>200</td>\n",
       "      <td>6.611323e-01</td>\n",
       "      <td>0.855118</td>\n",
       "      <td>9.743547e-01</td>\n",
       "      <td>100.0</td>\n",
       "      <td>6.965264e-01</td>\n",
       "      <td>0.891396</td>\n",
       "      <td>9.801586e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.177750</td>\n",
       "      <td>0.170053</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.467630e-10</td>\n",
       "      <td>6.314605e-12</td>\n",
       "      <td>8.970747e-11</td>\n",
       "      <td>6.323013e-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.9</td>\n",
       "      <td>l2_ball:scale=param_rms,base=1,mult=1e-12</td>\n",
       "      <td>200</td>\n",
       "      <td>9.950516e-01</td>\n",
       "      <td>0.999119</td>\n",
       "      <td>9.997933e-01</td>\n",
       "      <td>100.0</td>\n",
       "      <td>9.962287e-01</td>\n",
       "      <td>0.999316</td>\n",
       "      <td>9.998362e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.168976</td>\n",
       "      <td>0.169896</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.065305e-10</td>\n",
       "      <td>6.396818e-14</td>\n",
       "      <td>7.037172e-11</td>\n",
       "      <td>6.396818e-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.9</td>\n",
       "      <td>none</td>\n",
       "      <td>200</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.170325</td>\n",
       "      <td>0.168462</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.045493e-09</td>\n",
       "      <td>2.045493e-09</td>\n",
       "      <td>1.564998e-09</td>\n",
       "      <td>1.564998e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.9</td>\n",
       "      <td>nonneg</td>\n",
       "      <td>200</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.172126</td>\n",
       "      <td>0.171733</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.496424e-09</td>\n",
       "      <td>1.496424e-09</td>\n",
       "      <td>8.889774e-10</td>\n",
       "      <td>8.889774e-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.9</td>\n",
       "      <td>simplex:s=1.0</td>\n",
       "      <td>200</td>\n",
       "      <td>3.292186e+06</td>\n",
       "      <td>0.172943</td>\n",
       "      <td>6.584373e+08</td>\n",
       "      <td>100.0</td>\n",
       "      <td>1.424503e+06</td>\n",
       "      <td>0.172943</td>\n",
       "      <td>2.849005e+08</td>\n",
       "      <td>...</td>\n",
       "      <td>0.542510</td>\n",
       "      <td>0.997835</td>\n",
       "      <td>0.567103</td>\n",
       "      <td>0.169964</td>\n",
       "      <td>0.165664</td>\n",
       "      <td>6.0</td>\n",
       "      <td>9.712010e-02</td>\n",
       "      <td>1.022299e-01</td>\n",
       "      <td>9.712010e-02</td>\n",
       "      <td>1.022299e-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30 rows \u00d7 22 columns</p>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6bb121b6-f63f-49d1-bf1b-ab90d65bc7dc')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-6bb121b6-f63f-49d1-bf1b-ab90d65bc7dc button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-6bb121b6-f63f-49d1-bf1b-ab90d65bc7dc');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "    <div id=\"df-71667d25-a5e5-416a-93bc-ec730d594b56\">\n",
       "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-71667d25-a5e5-416a-93bc-ec730d594b56')\"\n",
       "                title=\"Suggest charts\"\n",
       "                style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "      </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "      <script>\n",
       "        async function quickchart(key) {\n",
       "          const quickchartButtonEl =\n",
       "            document.querySelector('#' + key + ' button');\n",
       "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "          try {\n",
       "            const charts = await google.colab.kernel.invokeFunction(\n",
       "                'suggestCharts', [key], {});\n",
       "          } catch (error) {\n",
       "            console.error('Error during call to suggestCharts:', error);\n",
       "          }\n",
       "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "        }\n",
       "        (() => {\n",
       "          let quickchartButtonEl =\n",
       "            document.querySelector('#df-71667d25-a5e5-416a-93bc-ec730d594b56 button');\n",
       "          quickchartButtonEl.style.display =\n",
       "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "        })();\n",
       "      </script>\n",
       "    </div>\n",
       "\n",
       "  <div id=\"id_05ab9482-0657-4cab-9a94-7e62e6acacdb\">\n",
       "    <style>\n",
       "      .colab-df-generate {\n",
       "        background-color: #E8F0FE;\n",
       "        border: none;\n",
       "        border-radius: 50%;\n",
       "        cursor: pointer;\n",
       "        display: none;\n",
       "        fill: #1967D2;\n",
       "        height: 32px;\n",
       "        padding: 0 0 0 0;\n",
       "        width: 32px;\n",
       "      }\n",
       "\n",
       "      .colab-df-generate:hover {\n",
       "        background-color: #E2EBFA;\n",
       "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "        fill: #174EA6;\n",
       "      }\n",
       "\n",
       "      [theme=dark] .colab-df-generate {\n",
       "        background-color: #3B4455;\n",
       "        fill: #D2E3FC;\n",
       "      }\n",
       "\n",
       "      [theme=dark] .colab-df-generate:hover {\n",
       "        background-color: #434B5C;\n",
       "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "        fill: #FFFFFF;\n",
       "      }\n",
       "    </style>\n",
       "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df')\"\n",
       "            title=\"Generate code using this dataframe.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "    <script>\n",
       "      (() => {\n",
       "      const buttonEl =\n",
       "        document.querySelector('#id_05ab9482-0657-4cab-9a94-7e62e6acacdb button.colab-df-generate');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      buttonEl.onclick = () => {\n",
       "        google.colab.notebook.generateWithVariable('df');\n",
       "      }\n",
       "      })();\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "dataframe",
       "variable_name": "df"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "    beta1                                  projection         score  \\\n",
       "0     0.0                                 box:l=0,u=1  0.000000e+00   \n",
       "1     0.0                             box:l=0,u=1e-08  0.000000e+00   \n",
       "2     0.0                               box:l=0,u=1e6  0.000000e+00   \n",
       "3     0.0  l2_ball:scale=param_rms,base=1,mult=0.0001  0.000000e+00   \n",
       "4     0.0   l2_ball:scale=param_rms,base=1,mult=1e-06  0.000000e+00   \n",
       "5     0.0                                        none  0.000000e+00   \n",
       "6     0.0                                      nonneg  0.000000e+00   \n",
       "7     0.0                             box:l=0,u=1e-09  1.111064e-02   \n",
       "8     0.0   l2_ball:scale=param_rms,base=1,mult=1e-08  1.111928e-01   \n",
       "9     0.0                             box:l=0,u=1e-10  1.234022e-01   \n",
       "10    0.0                             box:l=0,u=1e-11  5.512614e-01   \n",
       "11    0.0                             box:l=0,u=1e-12  1.399321e+00   \n",
       "12    0.0   l2_ball:scale=param_rms,base=1,mult=1e-10  1.420621e+00   \n",
       "13    0.0   l2_ball:scale=param_rms,base=1,mult=1e-12  1.990632e+00   \n",
       "14    0.0                               simplex:s=1.0  1.640396e+06   \n",
       "15    0.9                                 box:l=0,u=1  0.000000e+00   \n",
       "16    0.9                             box:l=0,u=1e-08  0.000000e+00   \n",
       "17    0.9                               box:l=0,u=1e6  0.000000e+00   \n",
       "18    0.9  l2_ball:scale=param_rms,base=1,mult=0.0001  0.000000e+00   \n",
       "19    0.9   l2_ball:scale=param_rms,base=1,mult=1e-06  0.000000e+00   \n",
       "20    0.9                                        none  0.000000e+00   \n",
       "21    0.9                                      nonneg  0.000000e+00   \n",
       "22    0.9                             box:l=0,u=1e-09  1.223317e-02   \n",
       "23    0.9                             box:l=0,u=1e-10  9.066582e-02   \n",
       "24    0.9   l2_ball:scale=param_rms,base=1,mult=1e-08  1.013029e-01   \n",
       "25    0.9                             box:l=0,u=1e-11  6.207002e-01   \n",
       "26    0.9                             box:l=0,u=1e-12  1.294380e+00   \n",
       "27    0.9   l2_ball:scale=param_rms,base=1,mult=1e-10  1.357659e+00   \n",
       "28    0.9   l2_ball:scale=param_rms,base=1,mult=1e-12  1.991280e+00   \n",
       "29    0.9                               simplex:s=1.0  4.716689e+06   \n",
       "\n",
       "     vr_rel_mean   vc_rel_mean  vr_active_%  vc_active_%  loss_end  \n",
       "0   0.000000e+00  0.000000e+00          0.0          0.0  0.169205  \n",
       "1   0.000000e+00  0.000000e+00          0.0          0.0  0.165977  \n",
       "2   0.000000e+00  0.000000e+00          0.0          0.0  0.168706  \n",
       "3   0.000000e+00  0.000000e+00          0.0          0.0  0.167198  \n",
       "4   0.000000e+00  0.000000e+00          0.0          0.0  0.170000  \n",
       "5   0.000000e+00  0.000000e+00          0.0          0.0  0.168290  \n",
       "6   0.000000e+00  0.000000e+00          0.0          0.0  0.169203  \n",
       "7   7.521221e-03  3.589418e-03        100.0        100.0  0.171322  \n",
       "8   6.818761e-02  4.300517e-02         99.5         99.5  0.172670  \n",
       "9   6.399449e-02  5.940768e-02        100.0        100.0  0.173588  \n",
       "10  2.815176e-01  2.697438e-01        100.0        100.0  0.172437  \n",
       "11  6.997876e-01  6.995329e-01        100.0        100.0  0.165856  \n",
       "12  7.008160e-01  7.198050e-01        100.0        100.0  0.174915  \n",
       "13  9.950806e-01  9.955513e-01        100.0        100.0  0.165134  \n",
       "14  1.070207e+06  5.701892e+05        100.0        100.0  0.168668  \n",
       "15  0.000000e+00  0.000000e+00          0.0          0.0  0.172115  \n",
       "16  0.000000e+00  0.000000e+00          0.0          0.0  0.170706  \n",
       "17  0.000000e+00  0.000000e+00          0.0          0.0  0.175025  \n",
       "18  0.000000e+00  0.000000e+00          0.0          0.0  0.172024  \n",
       "19  0.000000e+00  0.000000e+00          0.0          0.0  0.170602  \n",
       "20  0.000000e+00  0.000000e+00          0.0          0.0  0.168462  \n",
       "21  0.000000e+00  0.000000e+00          0.0          0.0  0.171733  \n",
       "22  6.924941e-03  5.308228e-03        100.0        100.0  0.174718  \n",
       "23  4.304976e-02  4.761606e-02        100.0        100.0  0.168975  \n",
       "24  6.108647e-02  4.021644e-02         99.5         99.5  0.173564  \n",
       "25  2.946465e-01  3.260538e-01        100.0        100.0  0.170100  \n",
       "26  6.485058e-01  6.458738e-01        100.0        100.0  0.173596  \n",
       "27  6.611323e-01  6.965264e-01        100.0        100.0  0.170053  \n",
       "28  9.950516e-01  9.962287e-01        100.0        100.0  0.169896  \n",
       "29  3.292186e+06  1.424503e+06        100.0        100.0  0.165664  "
      ],
      "text/html": [
       "\n",
       "  <div id=\"df-5b397e4d-5ccf-4bc0-9c3a-62fe86038bd6\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>beta1</th>\n",
       "      <th>projection</th>\n",
       "      <th>score</th>\n",
       "      <th>vr_rel_mean</th>\n",
       "      <th>vc_rel_mean</th>\n",
       "      <th>vr_active_%</th>\n",
       "      <th>vc_active_%</th>\n",
       "      <th>loss_end</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>box:l=0,u=1</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.169205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>box:l=0,u=1e-08</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.165977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>box:l=0,u=1e6</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.168706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>l2_ball:scale=param_rms,base=1,mult=0.0001</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.167198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>l2_ball:scale=param_rms,base=1,mult=1e-06</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.170000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0</td>\n",
       "      <td>none</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.168290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.0</td>\n",
       "      <td>nonneg</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.169203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.0</td>\n",
       "      <td>box:l=0,u=1e-09</td>\n",
       "      <td>1.111064e-02</td>\n",
       "      <td>7.521221e-03</td>\n",
       "      <td>3.589418e-03</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.171322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.0</td>\n",
       "      <td>l2_ball:scale=param_rms,base=1,mult=1e-08</td>\n",
       "      <td>1.111928e-01</td>\n",
       "      <td>6.818761e-02</td>\n",
       "      <td>4.300517e-02</td>\n",
       "      <td>99.5</td>\n",
       "      <td>99.5</td>\n",
       "      <td>0.172670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.0</td>\n",
       "      <td>box:l=0,u=1e-10</td>\n",
       "      <td>1.234022e-01</td>\n",
       "      <td>6.399449e-02</td>\n",
       "      <td>5.940768e-02</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.173588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.0</td>\n",
       "      <td>box:l=0,u=1e-11</td>\n",
       "      <td>5.512614e-01</td>\n",
       "      <td>2.815176e-01</td>\n",
       "      <td>2.697438e-01</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.172437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.0</td>\n",
       "      <td>box:l=0,u=1e-12</td>\n",
       "      <td>1.399321e+00</td>\n",
       "      <td>6.997876e-01</td>\n",
       "      <td>6.995329e-01</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.165856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.0</td>\n",
       "      <td>l2_ball:scale=param_rms,base=1,mult=1e-10</td>\n",
       "      <td>1.420621e+00</td>\n",
       "      <td>7.008160e-01</td>\n",
       "      <td>7.198050e-01</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.174915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.0</td>\n",
       "      <td>l2_ball:scale=param_rms,base=1,mult=1e-12</td>\n",
       "      <td>1.990632e+00</td>\n",
       "      <td>9.950806e-01</td>\n",
       "      <td>9.955513e-01</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.165134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.0</td>\n",
       "      <td>simplex:s=1.0</td>\n",
       "      <td>1.640396e+06</td>\n",
       "      <td>1.070207e+06</td>\n",
       "      <td>5.701892e+05</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.168668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.9</td>\n",
       "      <td>box:l=0,u=1</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.172115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.9</td>\n",
       "      <td>box:l=0,u=1e-08</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.170706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.9</td>\n",
       "      <td>box:l=0,u=1e6</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.175025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.9</td>\n",
       "      <td>l2_ball:scale=param_rms,base=1,mult=0.0001</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.172024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.9</td>\n",
       "      <td>l2_ball:scale=param_rms,base=1,mult=1e-06</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.170602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.9</td>\n",
       "      <td>none</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.168462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.9</td>\n",
       "      <td>nonneg</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.171733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.9</td>\n",
       "      <td>box:l=0,u=1e-09</td>\n",
       "      <td>1.223317e-02</td>\n",
       "      <td>6.924941e-03</td>\n",
       "      <td>5.308228e-03</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.174718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.9</td>\n",
       "      <td>box:l=0,u=1e-10</td>\n",
       "      <td>9.066582e-02</td>\n",
       "      <td>4.304976e-02</td>\n",
       "      <td>4.761606e-02</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.168975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.9</td>\n",
       "      <td>l2_ball:scale=param_rms,base=1,mult=1e-08</td>\n",
       "      <td>1.013029e-01</td>\n",
       "      <td>6.108647e-02</td>\n",
       "      <td>4.021644e-02</td>\n",
       "      <td>99.5</td>\n",
       "      <td>99.5</td>\n",
       "      <td>0.173564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.9</td>\n",
       "      <td>box:l=0,u=1e-11</td>\n",
       "      <td>6.207002e-01</td>\n",
       "      <td>2.946465e-01</td>\n",
       "      <td>3.260538e-01</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.170100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.9</td>\n",
       "      <td>box:l=0,u=1e-12</td>\n",
       "      <td>1.294380e+00</td>\n",
       "      <td>6.485058e-01</td>\n",
       "      <td>6.458738e-01</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.173596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.9</td>\n",
       "      <td>l2_ball:scale=param_rms,base=1,mult=1e-10</td>\n",
       "      <td>1.357659e+00</td>\n",
       "      <td>6.611323e-01</td>\n",
       "      <td>6.965264e-01</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.170053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.9</td>\n",
       "      <td>l2_ball:scale=param_rms,base=1,mult=1e-12</td>\n",
       "      <td>1.991280e+00</td>\n",
       "      <td>9.950516e-01</td>\n",
       "      <td>9.962287e-01</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.169896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.9</td>\n",
       "      <td>simplex:s=1.0</td>\n",
       "      <td>4.716689e+06</td>\n",
       "      <td>3.292186e+06</td>\n",
       "      <td>1.424503e+06</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.165664</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5b397e4d-5ccf-4bc0-9c3a-62fe86038bd6')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-5b397e4d-5ccf-4bc0-9c3a-62fe86038bd6 button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-5b397e4d-5ccf-4bc0-9c3a-62fe86038bd6');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "    <div id=\"df-b5ddd9e6-ddfb-4528-9d31-034174518ea5\">\n",
       "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-b5ddd9e6-ddfb-4528-9d31-034174518ea5')\"\n",
       "                title=\"Suggest charts\"\n",
       "                style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "      </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "      <script>\n",
       "        async function quickchart(key) {\n",
       "          const quickchartButtonEl =\n",
       "            document.querySelector('#' + key + ' button');\n",
       "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "          try {\n",
       "            const charts = await google.colab.kernel.invokeFunction(\n",
       "                'suggestCharts', [key], {});\n",
       "          } catch (error) {\n",
       "            console.error('Error during call to suggestCharts:', error);\n",
       "          }\n",
       "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "        }\n",
       "        (() => {\n",
       "          let quickchartButtonEl =\n",
       "            document.querySelector('#df-b5ddd9e6-ddfb-4528-9d31-034174518ea5 button');\n",
       "          quickchartButtonEl.style.display =\n",
       "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "        })();\n",
       "      </script>\n",
       "    </div>\n",
       "\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "dataframe",
       "summary": "{\n  \"name\": \"display(rank[[\\\"beta1\\\", \\\"projection\\\", \\\"score\\\", \\\"vr_rel_mean\\\", \\\"vc_rel_mean\\\", \\\"vr_active_%\\\", \\\"vc_active_%\\\", \\\"loss_end\\\"]])\",\n  \"rows\": 30,\n  \"fields\": [\n    {\n      \"column\": \"beta1\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.4576928649440469,\n        \"min\": 0.0,\n        \"max\": 0.9,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.9,\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"projection\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 15,\n        \"samples\": [\n          \"box:l=0,u=1e-10\",\n          \"box:l=0,u=1e-12\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"score\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 901932.1732254828,\n        \"min\": 0.0,\n        \"max\": 4716689.167746345,\n        \"num_unique_values\": 17,\n        \"samples\": [\n          0.0,\n          0.011110639326701251\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"vr_rel_mean\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 625588.9289387934,\n        \"min\": 0.0,\n        \"max\": 3292186.394706505,\n        \"num_unique_values\": 17,\n        \"samples\": [\n          0.0,\n          0.007521221057201426\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"vc_rel_mean\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 276785.52803495957,\n        \"min\": 0.0,\n        \"max\": 1424502.77303984,\n        \"num_unique_values\": 17,\n        \"samples\": [\n          0.0,\n          0.0035894182694998257\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"vr_active_%\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 50.71006157388872,\n        \"min\": 0.0,\n        \"max\": 100.0,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          0.0,\n          100.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"vc_active_%\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 50.71006157388872,\n        \"min\": 0.0,\n        \"max\": 100.0,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          0.0,\n          100.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"loss_end\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.002807567888073716,\n        \"min\": 0.16513362526893616,\n        \"max\": 0.17502467334270477,\n        \"num_unique_values\": 30,\n        \"samples\": [\n          0.1700526475906372,\n          0.17211522161960602\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
      }
     },
     "metadata": {}
    }
   ],
   "execution_count": 39,
   "source": [
    "# =========================\n",
    "# Adafactor Projection Test: SUMMARY (replace this whole cell)\n",
    "# =========================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def _safe(vals, fn, default=float(\"nan\")):\n",
    "    vals = [v for v in vals if not np.isnan(v)]\n",
    "    return float(fn(vals)) if len(vals) else default\n",
    "\n",
    "def _pct_active(vals, tol=1e-12):\n",
    "    vals = [v for v in vals if not np.isnan(v)]\n",
    "    if not vals:\n",
    "        return float(\"nan\")\n",
    "    return 100.0 * float(np.mean([v > tol for v in vals]))\n",
    "\n",
    "rows = []\n",
    "for run in all_runs:\n",
    "    metrics = run[\"metrics\"]\n",
    "\n",
    "    vr_pre_n = [m[\"vr_pre_norm\"] for m in metrics]\n",
    "    vr_post_n = [m[\"vr_post_norm\"] for m in metrics]\n",
    "    vc_pre_n = [m[\"vc_pre_norm\"] for m in metrics]\n",
    "    vc_post_n = [m[\"vc_post_norm\"] for m in metrics]\n",
    "    matched = [m[\"matched_states\"] for m in metrics]\n",
    "\n",
    "    vr_rel = [m[\"vr_rel\"] for m in metrics]\n",
    "    vc_rel = [m[\"vc_rel\"] for m in metrics]\n",
    "    vr_cos = [m[\"vr_cos\"] for m in metrics]\n",
    "    vc_cos = [m[\"vc_cos\"] for m in metrics]\n",
    "    losses = [m[\"loss\"] for m in metrics]\n",
    "\n",
    "    rows.append({\n",
    "        \"beta1\": run[\"beta1\"],\n",
    "        \"projection\": run[\"projection_spec\"],\n",
    "        \"steps\": len(metrics),\n",
    "\n",
    "        \"vr_rel_mean\": _safe(vr_rel, np.mean),\n",
    "        \"vr_rel_p95\":  _safe(vr_rel, lambda z: np.percentile(z, 95)),\n",
    "        \"vr_rel_max\":  _safe(vr_rel, np.max),\n",
    "        \"vr_active_%\": _pct_active(vr_rel),\n",
    "\n",
    "        \"vc_rel_mean\": _safe(vc_rel, np.mean),\n",
    "        \"vc_rel_p95\":  _safe(vc_rel, lambda z: np.percentile(z, 95)),\n",
    "        \"vc_rel_max\":  _safe(vc_rel, np.max),\n",
    "        \"vc_active_%\": _pct_active(vc_rel),\n",
    "\n",
    "        \"vr_cos_mean\": _safe(vr_cos, np.mean),\n",
    "        \"vr_cos_min\":  _safe(vr_cos, np.min),\n",
    "        \"vc_cos_mean\": _safe(vc_cos, np.mean),\n",
    "        \"vc_cos_min\":  _safe(vc_cos, np.min),\n",
    "\n",
    "        \"loss_start\": float(losses[0]) if losses else float(\"nan\"),\n",
    "        \"loss_end\":   float(losses[-1]) if losses else float(\"nan\"),\n",
    "        \"matched_states_mean\": _safe(matched, np.mean),\n",
    "\n",
    "        \"vr_pre_norm_mean\": _safe(vr_pre_n, np.mean),\n",
    "        \"vr_post_norm_mean\": _safe(vr_post_n, np.mean),\n",
    "        \"vc_pre_norm_mean\": _safe(vc_pre_n, np.mean),\n",
    "        \"vc_post_norm_mean\": _safe(vc_post_n, np.mean),\n",
    "\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(rows).sort_values(by=[\"beta1\", \"projection\"]).reset_index(drop=True)\n",
    "display(df)\n",
    "\n",
    "# Optional: simple ranking (lower mean rel-change = \"less intrusive projection\")\n",
    "rank = df.copy()\n",
    "rank[\"score\"] = rank[\"vr_rel_mean\"].fillna(0) + rank[\"vc_rel_mean\"].fillna(0)\n",
    "rank = rank.sort_values(by=[\"beta1\", \"score\"], ascending=[True, True]).reset_index(drop=True)\n",
    "display(rank[[\"beta1\", \"projection\", \"score\", \"vr_rel_mean\", \"vc_rel_mean\", \"vr_active_%\", \"vc_active_%\", \"loss_end\"]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "These blocks are running the mild projection we found to have impact on the Adafactor vectors, for a training size we will be able to compare to the ones without any projections."
   ],
   "metadata": {
    "id": "Ud4Cto6etv9f"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# =========================\n",
    "# Projection specs: control / mild / extreme\n",
    "# =========================\n",
    "\n",
    "# Sweeps you already ran (keep for short diagnostics & write-up)\n",
    "L2_MULTS_ALL = [1e-12, 1e-10, 1e-8, 1e-6, 1e-4]\n",
    "BOX_US_ALL   = [1e-12, 1e-11, 1e-10, 1e-9, 1e-8]\n",
    "\n",
    "CONTROL_SPECS = [\n",
    "    \"none\",\n",
    "]\n",
    "\n",
    "# Mild = activates but not overly destructive in your 200-step table:\n",
    "# - L2 mult=1e-8 looked \"mild\"\n",
    "# - Box u=1e-9 looked \"mild\"\n",
    "MILD_SPECS = [\n",
    "    \"l2_ball:scale=param_rms,base=1,mult=1e-8\",\n",
    "    \"box:l=0,u=1e-9\",\n",
    "]\n",
    "\n",
    "# Extreme = intentionally strong; keep for write-up but do NOT long-run\n",
    "EXTREME_SPECS = [\n",
    "    \"l2_ball:scale=param_rms,base=1,mult=1e-10\",\n",
    "    \"l2_ball:scale=param_rms,base=1,mult=1e-12\",\n",
    "    \"box:l=0,u=1e-11\",\n",
    "    \"box:l=0,u=1e-12\",\n",
    "    \"simplex:s=1.0\",   # optional if you want the \"stress test\" example\n",
    "]\n",
    "\n",
    "# Optional: keep the full short-sweep list (for quick tests only)\n",
    "SHORT_SWEEP_SPECS = (\n",
    "    CONTROL_SPECS\n",
    "    + [f\"l2_ball:scale=param_rms,base=1,mult={m}\" for m in L2_MULTS_ALL]\n",
    "    + [f\"box:l=0,u={u}\" for u in BOX_US_ALL]\n",
    ")\n",
    "\n",
    "# Long runs: only baseline + mild (these are the ones you actually train longer)\n",
    "LONG_RUN_SPECS = CONTROL_SPECS + MILD_SPECS\n",
    "\n",
    "print(\"CONTROL:\", CONTROL_SPECS)\n",
    "print(\"MILD:\", MILD_SPECS)\n",
    "print(\"EXTREME (no long runs):\", EXTREME_SPECS)\n",
    "print(\"LONG_RUN_SPECS:\", LONG_RUN_SPECS)\n"
   ],
   "metadata": {
    "id": "VJc-G-l9trJx",
    "outputId": "68fd5d3f-b48b-409d-fdfe-2df6fc2aa047",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "execution_count": 13,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "CONTROL: ['none']\n",
      "MILD: ['l2_ball:scale=param_rms,base=1,mult=1e-8', 'box:l=0,u=1e-9']\n",
      "EXTREME (no long runs): ['l2_ball:scale=param_rms,base=1,mult=1e-10', 'l2_ball:scale=param_rms,base=1,mult=1e-12', 'box:l=0,u=1e-11', 'box:l=0,u=1e-12', 'simplex:s=1.0']\n",
      "LONG_RUN_SPECS: ['none', 'l2_ball:scale=param_rms,base=1,mult=1e-8', 'box:l=0,u=1e-9']\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import os, json, time\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "# =========================\n",
    "# Persistent save location (Google Drive)\n",
    "# =========================\n",
    "BASE_DIR = \"/content/drive/MyDrive/adafactor_projection_test/\"\n",
    "RESULTS_DIR = os.path.join(BASE_DIR, \"long_runs\")\n",
    "\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "print(\"Saving long-run results to:\", RESULTS_DIR)\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Long run hyperparameters (comparable to sweep baseline)\n",
    "# =========================\n",
    "BATCH_SIZE = 128\n",
    "LR = 0.002\n",
    "WD = 3.125e-05\n",
    "\n",
    "EPOCHS_LONG = 30       # recommended for \"do we improve loss?\"\n",
    "MAX_STEPS_PER_EPOCH = None  # set to e.g. 200 if you want speed; otherwise full epoch\n",
    "\n",
    "\n",
    "\n",
    "def run_long(projection_spec: str, beta1: float, seed: int = 0):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    model = Autoencoder().to(device)\n",
    "    criterion = torch.nn.MSELoss()\n",
    "\n",
    "    # DataLoaders (assumes x_train, x_test exist)\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        torch.utils.data.TensorDataset(x_train),\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        drop_last=True,\n",
    "    )\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        torch.utils.data.TensorDataset(x_test),\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=False,\n",
    "        drop_last=False,\n",
    "    )\n",
    "\n",
    "    steps_per_epoch = len(train_loader) if MAX_STEPS_PER_EPOCH is None else min(len(train_loader), MAX_STEPS_PER_EPOCH)\n",
    "    total_steps = EPOCHS_LONG * steps_per_epoch\n",
    "\n",
    "    # Create optimizer (C1/C2 variants)\n",
    "    opt = create_optimizer(\n",
    "        \"C1\" if beta1 == 0.0 else \"C2\",\n",
    "        model,\n",
    "        epochs=EPOCHS_LONG,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        lr=LR,\n",
    "        wd=WD,\n",
    "        projection_spec=projection_spec,\n",
    "        track_projection_metrics=True,\n",
    "    )\n",
    "\n",
    "    def eval_loss():\n",
    "        model.eval()\n",
    "        losses = []\n",
    "        with torch.no_grad():\n",
    "            for (batch,) in test_loader:\n",
    "                batch = batch.to(device)\n",
    "                recon = model(batch)\n",
    "                loss = criterion(recon, batch)\n",
    "                losses.append(float(loss.item()))\n",
    "        model.train()\n",
    "        return float(np.mean(losses)) if losses else float(\"nan\")\n",
    "\n",
    "    history = []\n",
    "    t0 = time.time()\n",
    "\n",
    "    for epoch in range(EPOCHS_LONG):\n",
    "        model.train()\n",
    "        train_losses = []\n",
    "\n",
    "        for i, (batch,) in enumerate(train_loader):\n",
    "            if MAX_STEPS_PER_EPOCH is not None and i >= MAX_STEPS_PER_EPOCH:\n",
    "                break\n",
    "\n",
    "            batch = batch.to(device)\n",
    "            loss, grads, _ = autoencoder_oracle(model, criterion, batch, calc_hessian=False)\n",
    "\n",
    "            for p, g in zip(model.parameters(), grads):\n",
    "                p.grad = g\n",
    "\n",
    "            opt.step()\n",
    "            opt.zero_grad()\n",
    "            train_losses.append(float(loss.item()))\n",
    "\n",
    "        train_loss = float(np.mean(train_losses)) if train_losses else float(\"nan\")\n",
    "        test_loss = eval_loss()\n",
    "\n",
    "        history.append({\n",
    "            \"epoch\": epoch + 1,\n",
    "            \"train_loss\": train_loss,\n",
    "            \"test_loss\": test_loss,\n",
    "        })\n",
    "\n",
    "        # optional: print progress\n",
    "        if (epoch + 1) in (1, 5, 10, 20, 30, 50, 100) or (epoch + 1) == EPOCHS_LONG:\n",
    "            print(f\"[{projection_spec} | beta1={beta1}] epoch {epoch+1}/{EPOCHS_LONG} \"\n",
    "                  f\"train={train_loss:.6f} test={test_loss:.6f}\")\n",
    "\n",
    "    elapsed = time.time() - t0\n",
    "\n",
    "    # Save final weights\n",
    "    weights_path = os.path.join(\n",
    "        RESULTS_DIR,\n",
    "        f\"weights_beta1={beta1}_spec={projection_spec.replace(':','_').replace(',','_')}.pt\"\n",
    "    )\n",
    "    torch.save({\n",
    "        \"model_state_dict\": model.state_dict(),\n",
    "        \"seed\": seed,\n",
    "        \"projection_spec\": projection_spec,\n",
    "        \"beta1\": beta1,\n",
    "        \"lr\": LR,\n",
    "        \"wd\": WD,\n",
    "        \"batch_size\": BATCH_SIZE,\n",
    "        \"epochs\": EPOCHS_LONG,\n",
    "        \"max_steps_per_epoch\": MAX_STEPS_PER_EPOCH,\n",
    "    }, weights_path)\n",
    "\n",
    "    # Save JSON history (for plots / reproducibility)\n",
    "    out = {\n",
    "        \"meta\": {\n",
    "            \"seed\": seed,\n",
    "            \"beta1\": beta1,\n",
    "            \"projection_spec\": projection_spec,\n",
    "            \"lr\": LR,\n",
    "            \"wd\": WD,\n",
    "            \"batch_size\": BATCH_SIZE,\n",
    "            \"epochs\": EPOCHS_LONG,\n",
    "            \"steps_per_epoch_used\": steps_per_epoch,\n",
    "            \"elapsed_sec\": elapsed,\n",
    "            \"weights_path\": weights_path,\n",
    "        },\n",
    "        \"history\": history,\n",
    "        \"final\": {\n",
    "            \"train_loss\": history[-1][\"train_loss\"],\n",
    "            \"test_loss\": history[-1][\"test_loss\"],\n",
    "            \"best_test_loss\": min(h[\"test_loss\"] for h in history),\n",
    "            \"best_epoch\": int(np.argmin([h[\"test_loss\"] for h in history]) + 1),\n",
    "        }\n",
    "    }\n",
    "\n",
    "    json_path = os.path.join(\n",
    "        RESULTS_DIR,\n",
    "        f\"run_beta1={beta1}_spec={projection_spec.replace(':','_').replace(',','_')}.json\"\n",
    "    )\n",
    "    with open(json_path, \"w\") as f:\n",
    "        json.dump(out, f, indent=2)\n",
    "\n",
    "    return out, json_path, weights_path\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JyeCHy4MtKbo",
    "outputId": "c9219f4d-ed06-4e4b-8b10-41491ba0733e"
   },
   "execution_count": 19,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Using device: cuda\n",
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
      "Saving long-run results to: /content/drive/MyDrive/adafactor_projection_test/long_runs\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "all_long = []\n",
    "for beta1 in (0.0, 0.9):\n",
    "    for spec in LONG_RUN_SPECS:\n",
    "        out, jp, wp = run_long(spec, beta1, seed=0)\n",
    "        all_long.append((out, jp, wp))\n",
    "\n",
    "print(\"Done long runs. Saved to:\", RESULTS_DIR)\n"
   ],
   "metadata": {
    "collapsed": true,
    "id": "js3Pl9HkuGFQ",
    "outputId": "7c45785a-596b-42ef-b512-944f295fd07c",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "execution_count": 20,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[none | beta1=0.0] epoch 1/30 train=0.170627 test=0.169811\n",
      "[none | beta1=0.0] epoch 5/30 train=0.168837 test=0.167941\n",
      "[none | beta1=0.0] epoch 10/30 train=0.164713 test=0.163481\n",
      "[none | beta1=0.0] epoch 20/30 train=0.136115 test=0.133436\n",
      "[none | beta1=0.0] epoch 30/30 train=0.096238 test=0.094549\n",
      "[l2_ball:scale=param_rms,base=1,mult=1e-8 | beta1=0.0] epoch 1/30 train=0.170583 test=0.169725\n",
      "[l2_ball:scale=param_rms,base=1,mult=1e-8 | beta1=0.0] epoch 5/30 train=0.168453 test=0.167497\n",
      "[l2_ball:scale=param_rms,base=1,mult=1e-8 | beta1=0.0] epoch 10/30 train=0.163243 test=0.161799\n",
      "[l2_ball:scale=param_rms,base=1,mult=1e-8 | beta1=0.0] epoch 20/30 train=0.126145 test=0.123060\n",
      "[l2_ball:scale=param_rms,base=1,mult=1e-8 | beta1=0.0] epoch 30/30 train=0.087289 test=0.085730\n",
      "[box:l=0,u=1e-9 | beta1=0.0] epoch 1/30 train=0.170626 test=0.169809\n",
      "[box:l=0,u=1e-9 | beta1=0.0] epoch 5/30 train=0.168790 test=0.167869\n",
      "[box:l=0,u=1e-9 | beta1=0.0] epoch 10/30 train=0.162181 test=0.160164\n",
      "[box:l=0,u=1e-9 | beta1=0.0] epoch 20/30 train=0.103745 test=0.101569\n",
      "[box:l=0,u=1e-9 | beta1=0.0] epoch 30/30 train=0.089980 test=0.089178\n",
      "[none | beta1=0.9] epoch 1/30 train=0.170647 test=0.169830\n",
      "[none | beta1=0.9] epoch 5/30 train=0.168794 test=0.167872\n",
      "[none | beta1=0.9] epoch 10/30 train=0.164053 test=0.162690\n",
      "[none | beta1=0.9] epoch 20/30 train=0.130353 test=0.127330\n",
      "[none | beta1=0.9] epoch 30/30 train=0.089999 test=0.088414\n",
      "[l2_ball:scale=param_rms,base=1,mult=1e-8 | beta1=0.9] epoch 1/30 train=0.170591 test=0.169722\n",
      "[l2_ball:scale=param_rms,base=1,mult=1e-8 | beta1=0.9] epoch 5/30 train=0.168311 test=0.167323\n",
      "[l2_ball:scale=param_rms,base=1,mult=1e-8 | beta1=0.9] epoch 10/30 train=0.162380 test=0.160801\n",
      "[l2_ball:scale=param_rms,base=1,mult=1e-8 | beta1=0.9] epoch 20/30 train=0.121480 test=0.118314\n",
      "[l2_ball:scale=param_rms,base=1,mult=1e-8 | beta1=0.9] epoch 30/30 train=0.082354 test=0.080842\n",
      "[box:l=0,u=1e-9 | beta1=0.9] epoch 1/30 train=0.170646 test=0.169828\n",
      "[box:l=0,u=1e-9 | beta1=0.9] epoch 5/30 train=0.168745 test=0.167794\n",
      "[box:l=0,u=1e-9 | beta1=0.9] epoch 10/30 train=0.160851 test=0.158513\n",
      "[box:l=0,u=1e-9 | beta1=0.9] epoch 20/30 train=0.100846 test=0.098864\n",
      "[box:l=0,u=1e-9 | beta1=0.9] epoch 30/30 train=0.087047 test=0.085973\n",
      "Done long runs. Saved to: /content/drive/MyDrive/adafactor_projection_test/long_runs\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 2: Deep Training (100 epochs)\n",
    "\n",
    "Top 5 configs from Phase 1 for A1, B1, B3 - trained for 100 epochs with:\n",
    "- Checkpoints at epochs 25, 50, 75, 100\n",
    "- Logging: train/test loss, LR, gradient norm, weight norm\n",
    "- Reconstruction samples every 25 epochs"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Phase 2 configuration\n",
    "PHASE2_CONFIGS = {\n",
    "    'A1': [\n",
    "        {'lr_mult': 2, 'wd_mult': 0},\n",
    "        {'lr_mult': 1, 'wd_mult': 0},\n",
    "        {'lr_mult': 0.5, 'wd_mult': 0},\n",
    "        {'lr_mult': 0.25, 'wd_mult': 0},\n",
    "        {'lr_mult': 0.125, 'wd_mult': 0},\n",
    "    ],\n",
    "    'B1': [\n",
    "        {'lr_mult': 2, 'wd_mult': 4},\n",
    "        {'lr_mult': 2, 'wd_mult': 0.125},\n",
    "        {'lr_mult': 2, 'wd_mult': 2},\n",
    "        {'lr_mult': 2, 'wd_mult': 1},\n",
    "        {'lr_mult': 2, 'wd_mult': 0.5},\n",
    "    ],\n",
    "    'B3': [\n",
    "        {'lr_mult': 2, 'wd_mult': 0.5},\n",
    "        {'lr_mult': 2, 'wd_mult': 4},\n",
    "        {'lr_mult': 2, 'wd_mult': 0},\n",
    "        {'lr_mult': 2, 'wd_mult': 0.125},\n",
    "        {'lr_mult': 2, 'wd_mult': 0.25},\n",
    "    ],\n",
    "}\n",
    "\n",
    "PHASE2_EPOCHS = 100\n",
    "PHASE2_BATCH_SIZE = 128\n",
    "PHASE2_BASE_LR = 0.001\n",
    "PHASE2_BASE_WD = 0.001\n",
    "CHECKPOINT_EPOCHS = [25, 50, 75, 100]\n",
    "RECON_EPOCHS = [25, 50, 75, 100]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def run_phase2_training(variant_id, lr_mult, wd_mult, epochs=100, batch_size=128,\n",
    "                         base_lr=0.001, base_wd=0.001, checkpoint_epochs=[25,50,75,100],\n",
    "                         recon_epochs=[25,50,75,100], recon_images=None, save_dir=None):\n",
    "    \"\"\"Phase 2 training with extended logging and checkpoints.\"\"\"\n",
    "    import time\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    lr = base_lr * lr_mult\n",
    "    wd = base_wd * wd_mult\n",
    "\n",
    "    model = Autoencoder().to(device)\n",
    "    opt = create_optimizer(variant_id, model, epochs=epochs, batch_size=batch_size, lr=lr, wd=wd)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    x_train_dev = x_train.to(device)\n",
    "    x_test_dev = x_test.to(device)\n",
    "    train_loader = DataLoader(TensorDataset(x_train_dev), batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(TensorDataset(x_test_dev), batch_size=batch_size)\n",
    "\n",
    "    if recon_images is None:\n",
    "        recon_images = x_test_dev[:8]\n",
    "    else:\n",
    "        recon_images = recon_images.to(device)\n",
    "\n",
    "    history = {\n",
    "        'train_loss': [], 'test_loss': [], 'lr': [],\n",
    "        'grad_norm': [], 'weight_norm': [], 'epoch_time': [],\n",
    "        'reconstructions': {}\n",
    "    }\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        epoch_start = time.time()\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        epoch_grad_norm = 0\n",
    "\n",
    "        for batch in train_loader:\n",
    "            x = batch[0]\n",
    "            opt.zero_grad()\n",
    "            output = model(x)\n",
    "            loss = criterion(output, x)\n",
    "            loss.backward()\n",
    "\n",
    "            grad_norm = sum(p.grad.norm().item()**2 for p in model.parameters() if p.grad is not None)**0.5\n",
    "            epoch_grad_norm += grad_norm\n",
    "\n",
    "            opt.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        train_loss = epoch_loss / len(train_loader)\n",
    "        avg_grad_norm = epoch_grad_norm / len(train_loader)\n",
    "        weight_norm = sum(p.norm().item()**2 for p in model.parameters())**0.5\n",
    "\n",
    "        current_lr = lr\n",
    "        if hasattr(opt, 'scheduler') and opt.scheduler:\n",
    "            current_lr = opt.scheduler.get_last_lr()[0]\n",
    "        elif hasattr(opt, 'param_groups'):\n",
    "            current_lr = opt.param_groups[0].get('lr', lr)\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            test_loss = sum(criterion(model(b[0]), b[0]).item() for b in test_loader) / len(test_loader)\n",
    "\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['test_loss'].append(test_loss)\n",
    "        history['lr'].append(current_lr)\n",
    "        history['grad_norm'].append(avg_grad_norm)\n",
    "        history['weight_norm'].append(weight_norm)\n",
    "        history['epoch_time'].append(time.time() - epoch_start)\n",
    "\n",
    "        if epoch in recon_epochs:\n",
    "            with torch.no_grad():\n",
    "                recons = model(recon_images)\n",
    "            history['reconstructions'][epoch] = {\n",
    "                'originals': recon_images.cpu().numpy(),\n",
    "                'reconstructed': recons.cpu().numpy()\n",
    "            }\n",
    "\n",
    "        if epoch in checkpoint_epochs and save_dir:\n",
    "            ckpt_path = f\"{save_dir}/{variant_id}_lr{lr_mult}_wd{wd_mult}_epoch{epoch}.pt\"\n",
    "            torch.save(model.state_dict(), ckpt_path)\n",
    "            print(f\"  Saved checkpoint: {ckpt_path}\")\n",
    "\n",
    "        if epoch % 25 == 0 or epoch == 1:\n",
    "            print(f\"  Epoch {epoch}/{epochs}: train={train_loss:.6f}, test={test_loss:.6f}, \"\n",
    "                  f\"lr={current_lr:.2e}, grad_norm={avg_grad_norm:.4f}\")\n",
    "\n",
    "    history['total_time'] = time.time() - start_time\n",
    "    history['config'] = {'variant': variant_id, 'lr_mult': lr_mult, 'wd_mult': wd_mult,\n",
    "                         'lr': lr, 'wd': wd, 'epochs': epochs}\n",
    "\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# === SELECT WHICH VARIANTS TO RUN ===\n",
    "VARIANTS_TO_RUN = ['A1', 'B1', 'B3']  # Edit this list to run specific variants\n",
    "\n",
    "SAVE_DIR = '/content/drive/MyDrive/Optimization Project/results/phase2'\n",
    "import os\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "recon_images = x_test[:8]\n",
    "phase2_results = {}\n",
    "\n",
    "for variant_id in VARIANTS_TO_RUN:\n",
    "    if variant_id not in PHASE2_CONFIGS:\n",
    "        print(f\"Skipping {variant_id} - not in PHASE2_CONFIGS\")\n",
    "        continue\n",
    "    configs = PHASE2_CONFIGS[variant_id]\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Running {variant_id} ({len(configs)} configs)\")\n",
    "    print('='*60)\n",
    "\n",
    "    for i, cfg in enumerate(configs):\n",
    "        run_name = f\"{variant_id}_lr{cfg['lr_mult']}_wd{cfg['wd_mult']}\"\n",
    "        print(f\"\\n[{i+1}/{len(configs)}] {run_name}\")\n",
    "\n",
    "        model, history = run_phase2_training(\n",
    "            variant_id=variant_id,\n",
    "            lr_mult=cfg['lr_mult'],\n",
    "            wd_mult=cfg['wd_mult'],\n",
    "            epochs=PHASE2_EPOCHS,\n",
    "            batch_size=PHASE2_BATCH_SIZE,\n",
    "            base_lr=PHASE2_BASE_LR,\n",
    "            base_wd=PHASE2_BASE_WD,\n",
    "            checkpoint_epochs=CHECKPOINT_EPOCHS,\n",
    "            recon_epochs=RECON_EPOCHS,\n",
    "            recon_images=recon_images,\n",
    "            save_dir=SAVE_DIR\n",
    "        )\n",
    "        phase2_results[run_name] = history\n",
    "\n",
    "# Save results\n",
    "import json as json_module\n",
    "def convert_for_json(obj):\n",
    "    if isinstance(obj, dict):\n",
    "        return {k: convert_for_json(v) for k, v in obj.items() if k != 'reconstructions'}\n",
    "    elif isinstance(obj, list):\n",
    "        return [convert_for_json(v) for v in obj]\n",
    "    elif hasattr(obj, 'tolist'):\n",
    "        return obj.tolist()\n",
    "    return obj\n",
    "\n",
    "with open(f\"{SAVE_DIR}/phase2_results.json\", 'w') as f:\n",
    "    json_module.dump(convert_for_json(phase2_results), f, indent=2)\n",
    "print(f\"\\nSaved results to {SAVE_DIR}/phase2_results.json\")\n"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}